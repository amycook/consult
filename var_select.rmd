---
title: "Variable Selection"
author: "Amy Cook"
date: "Thursday, June 18, 2015"
output: word_document
---


```{r, echo=FALSE, include=FALSE}

library('knitr', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('ggplot2', lib = 'C:/Progra~1/R/R-3.2.1/library')
library("dplyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library("plyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library('magrittr',lib='C:/Progra~1/R/R-3.2.1/library')
library('reshape2',lib='C:/Progra~1/R/R-3.2.1/library')
library("rpart",lib = 'C:/Program Files/R/R-3.2.1/library')
library('car', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('e1071', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('corrgram', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('party', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('randomForest', lib = 'C:/Progra~1/R/R-3.2.1/library')

# library('knitr')
# library('ggplot2')
# library("dplyr")
# library("plyr")
# library('magrittr')
# library('reshape2')
# library("rpart")
# library('car')
# library('e1071')
# library('corrgram')
# library('party')

# setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
# setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
opts_knit$set(root.dir= "~/OneDrive/shared files/Bligh Tanner/masters/data")
```



```{r load_data, echo=FALSE}
all6a<- read.csv('all6a.csv')[,-1]
all6a$Start.Date<- as.Date(all6a$Start.Date)
all7<- read.csv('all7.csv')[,-1]
all7$Start.Date<- as.Date(all7$Start.Date)
```

Check for normality of return variable return per dollar
Outliers were deleted first

```{r}
# try deleting very high return.pdol values greater than 3
test<- all7 %>% filter(return.pdol <=3 & return.pdol> -2)
qqPlot(test$return.pdol)
qqPlot(log(test$return.pdol+.97))
#have a look at the shape of return.pdol as bar graph
ggplot(test, aes(x=return.pdol)) + geom_histogram(aes(y=..density..), binwidth=.1)
ggplot(test, aes(x=log(test$return.pdol+.97))) + geom_histogram(aes(y=..density..), binwidth=.1)
```

definietly don't want to use log - worse.
Let's look at standard deviation, mean, median, skewness, kurtosis
```{r}
sd(test$return.pdol)
mean(test$return.pdol)
median(test$return.pdol)
#mode
names(sort(-table(test$return.pdol)))[1]
skewness(test$return.pdol)
skewness(log(test$return.pdol+.97))
kurtosis(test$return.pdol)
```


skewness only 0.92!! for unlogged
skewness is -2.6 for logged data

kurtosis equals 2.85 - very peakedy - ie not flat. normal distribution is 0.

Create new data set that has outliers deleted, all7a

```{r}

### delete outliers! and variables that aren't by milestone
all7a<- all7 %>% filter(return.pdol <=3 & return.pdol> -2)
all7a<- all7 %>% select(-Paperless,-Innovation, -JobInvCount,-job.first.inv.email.Sent, 
                        -Total.Costs..AUD., -Stage, -Folders, -Total.Fee, -Disburse, -Subcon.fee,
                        -Job.Size, -Tot.Invoiced, -charge, -cost, -Dis.subcon, -hours,-profit,-balance,
                        -Role)

# write.csv(all7a,'all7a.csv')
all7a<- read.csv('all7a.csv')
```

# Variable Selection

It would be good to be able to narrow down the variables to the correlated or strongly affecting ones. This improves accuracy and speed of the final machine learning algoriths.

Anova and Linear regression will give us an idea of variable importance. Look at coefficients

In order to get Anova and linear regression to work on this data set, start with the variables that are complete or almost complete. Then can add additional variables one by one. The function below does this automatically. 

For lm, the output is ordered from abs(highest coefficient)

```{r, echo=FALSE}
BT.lin<- function(type= 'lm', data= all7a, add.var = 'Job.Type.Primary') {
        formula = paste("return.pdol ~ inv.mlsto + Discipline + client.count + Business +
                              Biz.size + Biz.type + Year + Num.days +
                              no.users + pc.contracttech + client.neginv + client.numinv + client.totinv +
                              pc.director + pc.midtech + pc.midpro + pc.gradpro + pc.seniortech + pc.seniorpro +
                              pc.pro + mean.peeps + hours.perday + num.inv + mean.inv + num.neginv + client.meaninv +
                              code.director + ProjEng.Pos", add.var, sep="+")
        if (type== 'aov') {
            model= aov(as.formula(formula)              
                      ,data= data)
            return(summary(model)) 
        }
        else{
                model=lm(as.formula(formula)                     
                        ,data= data)
                df<- as.data.frame(summary(model)$coefficients)
                df$var<-rownames(summary(model)$coefficients)
                colnames(df)[names(df) %in% 'Pr(>|t|)']<-'Pval'
                return(df %>% arrange(-abs(Estimate)))
                
        }
            
                
}
```

```{r}

# BT.lin(type= 'lm', data= all7a[1:1313,], add.var = 'Job.Type.Primary') %>% slice(1:20)
BT.lin(type= 'lm', data= all7a, add.var = 'Job.Type.Primary') %>% slice(1:20)
BT.lin(type= 'lm', data= all7a[1314:2283,], add.var = 'Job.Type.Primary') %>% slice(1:10)
BT.lin(type= 'aov', data= all7a, add.var = 'Job.Type.Primary')
BT.lin(type= 'aov', data= all7a[1314:2283,], add.var = 'Job.Type.Primary')

```

Most influential variables are:

* Business type
* director code
* Discipline
* Biz.type


Excluded variables include:

* Post.Code
* Billing.Type
* Job.Source
* Job.Detail.Primary
* Job.Detail.Secondary
* Job.Type.Primary
* Job.Type.Secondary
* State
* Type
* no.employees
* contact.count
* JD.Primary
* JD.Second
* dist
* timespan (won't know to begin with)
* Num.disc
* Inv.freq
* client.invfreq

# Correlation Analysis - Numeric variables with 'return.pdol' variable

First run Corrgram  - corrgram function
Run as type 'spearman' - non linear

```{r, echo=FALSE}

#### CORRELATION ANALYSIS

# str(all7a)
num.all<- all7a %>% select(client.count, no.employees, dist, charge.ph, no.users, pc.contracttech,
                           pc.director, pc.gradpro, pc.midpro, pc.midtech, pc.seniorpro, pc.seniortech, 
                           pc.unknown, pc.pro, pc.tech, Year, balance.mlsto,
                           timespan, Num.disc, Num.days, mean.peeps, hours.perday, hrs.mlsto, cost.mlsto, 
                           dis.sc.mlsto, 
                           inv.mlsto, return.pdol,
                           num.inv, mean.inv, Inv.freq, num.neginv, client.meaninv, client.invfreq, 
                           client.neginv, 
                           client.numinv,
                           client.totinv)
corrgram(num.all %>% select(client.count, charge.ph, no.users, pc.director, pc.gradpro, pc.midpro, 
                            pc.midtech, pc.contracttech, return.pdol,
                            pc.seniorpro, pc.seniortech, balance.mlsto,
                            pc.unknown, pc.pro, pc.tech, Year, Num.days, mean.peeps, hours.perday, hrs.mlsto, 
                            cost.mlsto,
                            dis.sc.mlsto, inv.mlsto, num.inv, mean.inv, client.meaninv, client.totinv,
                            timespan) %>% filter(complete.cases(.)), order=T, 
         lower.panel = panel.shade,
         upper.panel = panel.pie,
         text.panel = panel.txt,
         cor.method = 'spearman')
# summary(num.all)
# num.all[rowSums(is.na(num.all))>0,]
```

Now output cor.test results for all numeric variables using both pearson (linear) and kendall (non-linear) to compare.
Kendall does not have problems with ties

```{r, echo=FALSE}

#correlation by each variable against return.perdol individually - numeric variables only

cor.all<- c("client.count", "no.users", "pc.director", "pc.gradpro", "pc.midpro", 
            "pc.midtech", "pc.contracttech", "return.pdol","Inv.freq", 'num.neginv','client.invfreq','client.neginv',
            "pc.seniorpro", "pc.seniortech", "no.employees", "dist", "Num.disc",'client.numinv',
            "pc.unknown", "pc.pro", "pc.tech", "Year", "Num.days", "mean.peeps", "hours.perday", "hrs.mlsto", 
            "cost.mlsto",
            "dis.sc.mlsto", "inv.mlsto", "num.inv", "mean.inv", "client.meaninv", "client.totinv",
            "timespan")

summ<- data.frame("test"=NULL,
                  'var' = NULL,
                  'corr' = NULL,
                  'p.val' = NULL)

for(i in 1:length(cor.all)) {
        
        a = cor.test(all7a$return.pdol, all7a[,cor.all[i]], method = c('pearson'))

        b = cor.test(all7a$return.pdol, all7a[,cor.all[i]], method = c('kendall'))
        
        inter.summ = data.frame('test'= c('pearson','kendall'),
                                'var' = c(cor.all[i], cor.all[i]),
                                'corr' = c(a$estimate, b$estimate),
                                'p.val' = c(a$p.val, b$p.val))
        
        summ = rbind(summ,inter.summ)
}

summ$p.val <- format(summ$p.val, scientific=TRUE, digits = 3)
summ$p.val <- as.numeric(summ$p.val)
summ$corr <- round(summ$corr, 3)

```

Output ordered by p value
```{r, echo=TRUE}

summ %>% arrange(test, p.val)

```

Output ordered by correlation value - only 'significant' results to 5%

```{r}

summ %>% filter(p.val<= 0.05) %>% arrange(test, -abs(corr))

```

From this list it can be concluded what the most influential variables are. 
For the purposes of return.pdol prediction, cost.mlsto, hrs.mlsto, Num.days, dis.sc.mlsto,
num.neginv, Year (won't help in the future) cannot be known. 

This leaves the following important variables:

```{r, echo=FALSE}
kable(
        summ %>% filter(p.val<= 0.05, test== 'kendall') %>% arrange(test, -abs(corr)) %>% 
                select(var, p.val, corr) %>%
                filter(var!= 'cost.mlsto', var!= 'hrs.mlsto', var!= 'Num.days', var!= 'dis.sc.mlsto',
                       var!= 'num.neginv',var!= 'Year', var!= 'return.pdol')
         )

```



# Strength of association with Categorical variables

Use cforest from party package - (Strobl etc, 2007)

* must be subsampling without replacement
* subsampling size 0.632 times recommended because about 63.2% of data end up in bootstrap sample
* cforest defauly is unbiased - for unbiased variable importance as per Strobl etc 2007
* since variables are potentially correlated to one another, need to set conditional permutation importance varimp(obj, conditional=TRUE)
    * This iterates each variable against correlated variables in a controlled way. Ie the target variable is assessed against subgroups of the data where a correlated variable is constant.
    
For cforest, NA values are not handled. Therefore create a 'core' dataframe consisting of the variables which have very low NA entries (say max 10) and delete all NA rows or impute.

State- impute QLD
Business - 22 NA values - impute 'unknown'
Biz.size - 38 NA's - find median, impute?
Biz.type - 18 NA's impute???
charge.ph - delete
charge.pc - delete
Start.Date - delete
End.Date - delete
no.users - 4 NA values - impute
pc.usersss - 7 NA's - what to do. 0 for all? averages for all?
Num.disc - 84 NA's - impute 1?
num.inv and mean.inv- 1 NA?



```{r core_creation, include=FALSE}
#impute NA values in some variables to be in core, call this all7b
all7b<- all7a
all7b[is.na(all7b$State),]$State <- 'QLD'
#RSL Care client - now have details
all7b[all7b$code.client %in% 'C2835',]$no.employees <- 1001
all7b[all7b$code.client %in% 'C2835',]$Biz.type <- 'NFP'
all7b[all7b$code.client %in% 'C2835',]$Business <- 'developer/real estate'
all7b[all7b$code.client %in% 'C2758',]$Biz.size <- 'national'
all7b[all7b$code.client %in% 'C2758',]$no.employees <- 2
all7b[all7b$code.client %in% 'C2758',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2758',]$Business <- 'business'
all7b[all7b$code.client %in% 'C2326',]$Business <- 'health'
all7b[all7b$code.client %in% 'C2338',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2338',]$Business <- 'developer/real estate'
all7b[all7b$code.client %in% 'C2338',]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2882',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2882',]$Business <- 'builder'
all7b[all7b$code.client %in% 'C2882',]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2882',]$no.employees <- 3
#all remaining NA Business will be 'unknown'
all7b$Business<- as.character(all7b$Business)
all7b[is.na(all7b$Business),]$Business <- 'unknown'
all7b$Business<- as.factor(all7b$Business)

#impute Biz.size to be 'local' by default - this is most popular category
all7b[is.na(all7b$Biz.size),]$Biz.size <- 'local'
#impute Biz.type to be 'private' by default - this is most popular category
#client which is 'department of national parks' shall be 'gov'
all7b[is.na(all7b$Biz.type),]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2503',]$Biz.type <- 'gov'
#delete charge.ph and charge.pc, Start.Date and End.Date
all7b<- all7b %>% select(-charge.ph, -charge.pc, -Start.Date, -End.Date)
#impute NA no.users as 2 = median and mode
all7b[is.na(all7b$no.users),]$no.users <- 2
#impute NA num.disc as 1 - overwhelmingly the most common
all7b[is.na(all7b$Num.disc),]$Num.disc <- 1
#num.inv and mean.inv both have a single NA value
# this consisted of single large -ve invoice. I think we should delete this case 
all7b<- all7b %>% filter(!is.na(num.inv))
# 7 NA cases with significant dis.sc.mlsto - ie done by contractor 
pc<- c('pc.contracttech','pc.director', 'pc.midtech', 'pc.midpro', 'pc.gradpro', 'pc.seniortech', 'pc.seniorpro', 'pc.pro', 'pc.tech')

for (i in 1:length(pc)){
        all7b[is.na(all7b[,pc[i]]),][,pc[i]] <-0
}

#change unknown Num.disc to be 1
all7b[is.na(all7b$Num.disc),]$Num.disc <- 1
# write.csv(all7b,'all7b.csv')

 
```

Start running cforests
Very slow, start with 100 rows only, using only core variables

```{r, echo=FALSE}
all7b<- read.csv('all7b.csv')

#create core data frame - variables with very few NA's and delete these NA's

core<- all7b %>% select(code.client , Discipline ,  client.count , inv.mlsto , Business ,
        Biz.size , Biz.type , no.users , Year , timespan , Num.disc , Num.days , mean.peeps,
        pc.contracttech , hours.perday , profit.mlsto , balance.mlsto , hrs.mlsto , 
        cost.mlsto , dis.sc.mlsto , inv.mlsto , return.pdol , code.contact ,
        pc.director , pc.midtech , pc.midpro , pc.gradpro , pc.seniortech , pc.seniorpro ,
        pc.pro , code.director , code.ProjEng , ProjEng.Pos , num.inv , mean.inv , num.neginv , 
        client.meaninv , client.neginv , client.numinv , client.totinv)

mini.core2 <- all7b %>% select(code.client , Discipline ,  client.count , inv.mlsto , Business ,
        Biz.size , Biz.type , no.users , timespan, Num.disc, 
        return.pdol, code.contact,
        pc.director,
        pc.pro , code.director , ProjEng.Pos ,
        client.meaninv , client.neginv , client.numinv , client.totinv)

mini.core3 <- all7b %>%
        select(Discipline , inv.mlsto , Business,
        Biz.size , Biz.type , no.users , timespan, Num.disc, 
        return.pdol, pc.director,
        pc.pro , code.director , ProjEng.Pos,
        client.meaninv , client.neginv , client.numinv , client.totinv)

```

Try to make all cforests random selection of 200 rows - this time post 2009


```{r, echo=FALSE}

system.time(
all.cfor2<- cforest(return.pdol~.,
        data= mini.core3
        )
)

var.imp4<- varimp(all.cfor) %>% as.data.frame
colnames(var.imp4)[names(var.imp4) %in% '.']<-'var.imp'
var.imp4$var <- rownames(var.imp4)
#reorder levels for bar plot
var.imp4 <- within(var.imp4, 
                    var <- factor(var,levels=
                                            var.imp4[order(-abs(var.imp4$var.imp)),]$var))

q<- ggplot(data=var.imp4, aes(x=var, y=var.imp)) + theme(axis.text.x=element_text(angle=45,hjust=1))
q + geom_bar(stat='identity', position = 'dodge')

save(all.cfor,file= 'C:/Users/n9232371/Documents/github/consult/object/all_cfor.RData')
load('C:/Users/n9232371/Documents/github/consult/object/all_cfor.RData')
```

From analysing all jobs including 2009 and beyond, the most important variables are:
`levels(var.imp4$var)[1:10] %>% as.data.frame`

The least important are 
`levels(var.imp4$var)[11:19]`

Let's compare this to 

* all data from 2002 to 2014 - currently running in another R window
* ANOVA - all data because fast
* random forest

## Anova:

```{r anova_varimp, echo=FALSE}
#use mini.core2- all years
system.time(
        aov.all<- aov(return.pdol~.,
                      data= mini.core3)
        )


summary(aov.all)
```

Therefore most significant variables are:

* no.users - number of people on the job - <<.1%
* code.director - <<.1%
* timespan - 4%
* Num.disc - 5.6%
* Discipline - 5.65%

worst: Business, ProjEng.Pos, client.numinv, 

Contrasts to cforest results include

* code.client pval=1 - this is likely because code.contact and code.client are highly correlated
* inv.mlsto pval= .721 - surprising
* client.numinv pval = 0.923 - surprisingly different

# random forests

First optimise parameters mtry and ntree - exclude code.client and code.contact for now as they have way too many categories

```{r randomForest, echo=FALSE}
#random forest can have max 53 categories in a variable
#code.client has the most categories - 664

# find optimal mtry

#create train/test sets

train_ind <- sample(seq_len(2355), size = 1570)

train <- mini.core4[train_ind, ]
test <- mini.core4[-train_ind, ]
oob.err<-double(13)
test.err<-double(13)
for(mtry in 1:13){
        fit<-randomForest(return.pdol ~., data=train, mtry=mtry, ntree=400)
        oob.err[mtry]<-fit$mse[400]
        test.err[mtry]<- mean((predict(fit,test)-test$return.pdol)^2)
        cat(mtry," ")
}

qplot(1:mtry,test.err,geom=c("point","line"),color="pink")+
        geom_line(aes(1:mtry,oob.err,colour="blue"))+
        geom_point(aes(1:mtry,oob.err, colour="blue"))

#COMPLETELY FLAT - MAY AS WELL USE MTRY = 5????? NOW VARY NUMBER OF TREES

trees<- c(300,500,700,1000,1500)
oob.err<-double(5)
test.err<-double(5)
for(ntree in 1:5){
        fit<-randomForest(return.pdol ~., data=train, mtry=5, ntree=trees[ntree])
        oob.err[ntree]<-fit$mse[trees[ntree]]
        test.err[ntree]<- mean((predict(fit,test)-test$return.pdol)^2)
        cat(ntree," ")
}

qplot(1:ntree,test.err,geom=c("point","line"),color="pink")+
        geom_line(aes(1:ntree,oob.err,colour="blue"))+
        geom_point(aes(1:ntree,oob.err, colour="blue"))

#again, doesn't matter!! line is dead flat. i'm not having much confidence in the predictiveness of this model. 
```

Extract important variables from randomForest loop

423 of the 664 client companies and 485 of the 670 client contacts appear ONLY once in the entire data set.
While they may be great predictors what do we do in the majority of cases where the contact is new!?

```{r, echo=FALSE}
set.seed(47)

#try running without code.client or code.contact
multi.forest1<-function(data=mini.core3, nforest=1:10){
        sample=double(50)
        imp = NULL
        for (i in 1:length(nforest)){
                
                #run tree
                assign(paste("Forest",i,sep=""),randomForest(return.pdol~.,
                                                             data=data,
                                                             ntree=400,mtry=5, importance=TRUE),
                       envir=.GlobalEnv)
                
                #put together importance rank for each variable
                
                rand.imp= importance(paste("Forest",i,sep="") %>% get, type=1, scale=FALSE) %>% as.data.frame
                colnames(rand.imp)[names(rand.imp) %in% '%IncMSE']= 'IncMSE'
                rand.imp$var= rownames(rand.imp)
                rand.imp= rand.imp %>% arrange(-abs(IncMSE))
                rand.imp$rank= 1:nrow(rand.imp)
                
                #rbind all variables and ranks on top of one another for each Forest
                
                imp = rbind(imp, rand.imp %>% select(var,rank))

                cat(i," ")
                
        }
        
        #output average rank of each variable
        rank = ddply(imp, .(var), summarise, m.rank =mean(rank)) %>% arrange(m.rank)
        return(rank)
        
}

rand.rank<- multi.forest1(data=mini.core2 %>% select(-code.contact, -code.client), nforest=1:10)

kable(rand.rank)

```

# New Variable - client age

```{r, echo=FALSE}
#read csv file with all the client details - want to merge this with client df
details<- read.csv('17marchclients.csv',
                   na.strings = c("", " ", "NA"))
head(details)
#delete client.contact, client.count, and contact.count columns
details<- details[,c(1,5:9)]
#rename columns
names(details)[2]<- 'Business'
names(details)[4]<- 'Biz.size'
names(details)[5]<- 'Biz.type'
names(details)[6]<- 'Biz.age'

#delete rows with 4 NA values
details<-details[rowSums(is.na(details))!=5,]
details<-details[rowSums(is.na(details))!=4,]

#add row for client code
code.client<- read.csv('code_client2.csv')[,-1]
details<- merge(details, code.client, by.x ='client', by.y ='client2', all.x=TRUE, all.y=FALSE)
details<- transform(details, client.age = 2014- Biz.age)

#merge client.age column of details to all7b by code.client
all7b<- merge(all7b, details %>% select(code, client.age), by.x = 'code.client', by.y = 'code', all.x=TRUE, all.y=FALSE)

```

See how variable selection goes with random forest first, then probably just run cforest again without code.client and contact.code

Use cforest to test remaining variables (with significant NA components)

These include:

* code.client
* Post.Code
* Billing.Type
* Job.Source
* Job.Detail.Primary
* Job.Detail.Secondary
* Job.Type.Primary
* Job.Type.Secondary
* Type
* Project.Value
* no.employees
* client age????
* contact.count
* JD.Primary
* JD.Second
* dist
* code.contact
* client.age

First eliminate some of the job.detail.secondary - that kind of thing
Then see if any have matching NA rows
Then run cforest on each variable individually
















