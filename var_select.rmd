---
title: "Variable Selection"
author: "Amy Cook"
date: "Thursday, June 18, 2015"
output: word_document
---


```{r, echo=FALSE, include=FALSE}

# library('knitr', lib = 'C:/Progra~1/R/R-3.1.2/library')
# library('ggplot2', lib = 'C:/Progra~1/R/R-3.1.2/library')
# library("dplyr",lib = 'C:/Progra~1/R/R-3.1.2/library')
# library("plyr",lib = 'C:/Progra~1/R/R-3.1.2/library')
# library('magrittr',lib='C:/Progra~1/R/R-3.1.3/library')
# library('reshape2',lib='C:/Progra~1/R/R-3.1.3/library')
# library("rpart",lib = 'C:/Program Files/R/R-3.2.0/library')
# library('car', lib = 'C:/Progra~1/R/R-3.2.0/library')
# library('e1071', lib = 'C:/Progra~1/R/R-3.2.0/library')
# library('corrgram', lib = 'C:/Progra~1/R/R-3.2.0/library')
# library('party', lib = 'C:/Progra~1/R/R-3.2.0/library')

library('knitr')
library('ggplot2')
library("dplyr")
library("plyr")
library('magrittr')
library('reshape2')
library("rpart")
library('car')
library('e1071')
library('corrgram')
library('party')

# setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
opts_knit$set(root.dir= "~/OneDrive/shared files/Bligh Tanner/masters/data")
```



```{r load_data, echo=FALSE}
all6a<- read.csv('all6a.csv')[,-1]
all6a$Start.Date<- as.Date(all6a$Start.Date)
all7<- read.csv('all7.csv')[,-1]
all7$Start.Date<- as.Date(all7$Start.Date)
```

Check for normality of return variable return per dollar
Outliers were deleted first

```{r}
# try deleting very high return.pdol values greater than 3
test<- all7 %>% filter(return.pdol <=3 & return.pdol> -2)
qqPlot(test$return.pdol)
qqPlot(log(test$return.pdol+.97))
#have a look at the shape of return.pdol as bar graph
ggplot(test, aes(x=return.pdol)) + geom_histogram(aes(y=..density..), binwidth=.1)
ggplot(test, aes(x=log(test$return.pdol+.97))) + geom_histogram(aes(y=..density..), binwidth=.1)
```

definietly don't want to use log - worse.
Let's look at standard deviation, mean, median, skewness, kurtosis
```{r}
sd(test$return.pdol)
mean(test$return.pdol)
median(test$return.pdol)
#mode
names(sort(-table(test$return.pdol)))[1]
skewness(test$return.pdol)
skewness(log(test$return.pdol+.97))
kurtosis(test$return.pdol)
```


skewness only 0.92!! for unlogged
skewness is -2.6 for logged data

kurtosis equals 2.85 - very peakedy - ie not flat. normal distribution is 0.

Create new data set that has outliers deleted, all7a

```{r}

### delete outliers! and variables that aren't by milestone
all7a<- all7 %>% filter(return.pdol <=3 & return.pdol> -2)
all7a<- all7 %>% select(-Paperless,-Innovation, -JobInvCount,-job.first.inv.email.Sent, 
                        -Total.Costs..AUD., -Stage, -Folders, -Total.Fee, -Disburse, -Subcon.fee,
                        -Job.Size, -Tot.Invoiced, -charge, -cost, -Dis.subcon, -hours,-profit,-balance,
                        -Role)

```

# Variable Selection

It would be good to be able to narrow down the variables to the correlated or strongly affecting ones. This improves accuracy and speed of the final machine learning algoriths.

Anova and Linear regression will give us an idea of variable importance. Look at coefficients

In order to get Anova and linear regression to work on this data set, start with the variables that are complete or almost complete. Then can add additional variables one by one. The function below does this automatically. 

For lm, the output is ordered from abs(highest coefficient)

```{r, echo=FALSE}
BT.lin<- function(type= 'lm', data= all7a, add.var = 'Job.Type.Primary') {
        formula = paste("return.pdol ~ inv.mlsto + Discipline + client.count + Business +
                              Biz.size + Biz.type + Year + Num.days +
                              no.users + pc.contracttech + client.neginv + client.numinv + client.totinv +
                              pc.director + pc.midtech + pc.midpro + pc.gradpro + pc.seniortech + pc.seniorpro +
                              pc.pro + mean.peeps + hours.perday + num.inv + mean.inv + num.neginv + client.meaninv +
                              code.director + ProjEng.Pos", add.var, sep="+")
        if (type== 'aov') {
            model= aov(as.formula(formula)              
                      ,data= data)
            return(summary(model)) 
        }
        else{
                model=lm(as.formula(formula)                     
                        ,data= data)
                df<- as.data.frame(summary(model)$coefficients)
                df$var<-rownames(summary(model)$coefficients)
                colnames(df)[names(df) %in% 'Pr(>|t|)']<-'Pval'
                return(df %>% arrange(-abs(Estimate)))
                
        }
            
                
}
```

```{r}

# BT.lin(type= 'lm', data= all7a[1:1313,], add.var = 'Job.Type.Primary') %>% slice(1:20)
BT.lin(type= 'lm', data= all7a, add.var = 'Job.Type.Primary') %>% slice(1:20)
BT.lin(type= 'lm', data= all7a[1314:2283,], add.var = 'Job.Type.Primary') %>% slice(1:10)
BT.lin(type= 'aov', data= all7a, add.var = 'Job.Type.Primary')
BT.lin(type= 'aov', data= all7a[1314:2283,], add.var = 'Job.Type.Primary')

```

Most influential variables are:

* Business type
* director code
* Discipline
* Biz.type


Excluded variables include:

* Post.Code
* Billing.Type
* Job.Source
* Job.Detail.Primary
* Job.Detail.Secondary
* Job.Type.Primary
* Job.Type.Secondary
* State
* Type
* no.employees
* contact.count
* JD.Primary
* JD.Second
* dist
* timespan (won't know to begin with)
* Num.disc
* Inv.freq
* client.invfreq

# Correlation Analysis - Numeric variables with 'return.pdol' variable

First run Corrgram  - corrgram function
Run as type 'spearman' - non linear

```{r, echo=FALSE}

#### CORRELATION ANALYSIS

# str(all7a)
num.all<- all7a %>% select(client.count, no.employees, dist, charge.ph, no.users, pc.contracttech,
                           pc.director, pc.gradpro, pc.midpro, pc.midtech, pc.seniorpro, pc.seniortech, 
                           pc.unknown, pc.pro, pc.tech, Year, balance.mlsto,
                           timespan, Num.disc, Num.days, mean.peeps, hours.perday, hrs.mlsto, cost.mlsto, 
                           dis.sc.mlsto, 
                           inv.mlsto, return.pdol,
                           num.inv, mean.inv, Inv.freq, num.neginv, client.meaninv, client.invfreq, 
                           client.neginv, 
                           client.numinv,
                           client.totinv)
corrgram(num.all %>% select(client.count, charge.ph, no.users, pc.director, pc.gradpro, pc.midpro, 
                            pc.midtech, pc.contracttech, return.pdol,
                            pc.seniorpro, pc.seniortech, balance.mlsto,
                            pc.unknown, pc.pro, pc.tech, Year, Num.days, mean.peeps, hours.perday, hrs.mlsto, 
                            cost.mlsto,
                            dis.sc.mlsto, inv.mlsto, num.inv, mean.inv, client.meaninv, client.totinv,
                            timespan) %>% filter(complete.cases(.)), order=T, 
         lower.panel = panel.shade,
         upper.panel = panel.pie,
         text.panel = panel.txt,
         cor.method = 'spearman')
# summary(num.all)
# num.all[rowSums(is.na(num.all))>0,]
```

Now output cor.test results for all numeric variables using both pearson (linear) and kendall (non-linear) to compare.
Kendall does not have problems with ties

```{r, echo=FALSE}

#correlation by each variable against return.perdol individually - numeric variables only

cor.all<- c("client.count", "no.users", "pc.director", "pc.gradpro", "pc.midpro", 
            "pc.midtech", "pc.contracttech", "return.pdol","Inv.freq", 'num.neginv','client.invfreq','client.neginv',
            "pc.seniorpro", "pc.seniortech", "no.employees", "dist", "Num.disc",'client.numinv',
            "pc.unknown", "pc.pro", "pc.tech", "Year", "Num.days", "mean.peeps", "hours.perday", "hrs.mlsto", 
            "cost.mlsto",
            "dis.sc.mlsto", "inv.mlsto", "num.inv", "mean.inv", "client.meaninv", "client.totinv",
            "timespan")

summ<- data.frame("test"=NULL,
                  'var' = NULL,
                  'corr' = NULL,
                  'p.val' = NULL)

for(i in 1:length(cor.all)) {
        
        a = cor.test(all7a$return.pdol, all7a[,cor.all[i]], method = c('pearson'))

        b = cor.test(all7a$return.pdol, all7a[,cor.all[i]], method = c('kendall'))
        
        inter.summ = data.frame('test'= c('pearson','kendall'),
                                'var' = c(cor.all[i], cor.all[i]),
                                'corr' = c(a$estimate, b$estimate),
                                'p.val' = c(a$p.val, b$p.val))
        
        summ = rbind(summ,inter.summ)
}

summ$p.val <- format(summ$p.val, scientific=TRUE, digits = 3)
summ$p.val <- as.numeric(summ$p.val)
summ$corr <- round(summ$corr, 3)

```

Output ordered by p value
```{r, echo=TRUE}

summ %>% arrange(test, p.val)

```

Output ordered by correlation value - only 'significant' results to 5%

```{r}

summ %>% filter(p.val<= 0.05) %>% arrange(test, -abs(corr))

```

From this list it can be concluded what the most influential variables are. 
For the purposes of return.pdol prediction, cost.mlsto, hrs.mlsto, Num.days, dis.sc.mlsto,
num.neginv, Year (won't help in the future) cannot be known. 

This leaves the following important variables:

```{r, echo=FALSE}
kable(
        summ %>% filter(p.val<= 0.05, test== 'kendall') %>% arrange(test, -abs(corr)) %>% 
                select(var, p.val, corr) %>%
                filter(var!= 'cost.mlsto', var!= 'hrs.mlsto', var!= 'Num.days', var!= 'dis.sc.mlsto',
                       var!= 'num.neginv',var!= 'Year', var!= 'return.pdol')
         )

```



# Strength of association with Categorical variables

Use cforest from party package - (Strobl etc, 2007)

* must be subsampling without replacement
* subsampling size 0.632 times recommended because about 63.2% of data end up in bootstrap sample
* cforest defauly is unbiased - for unbiased variable importance as per Strobl etc 2007
* since variables are potentially correlated to one another, need to set conditional permutation importance varimp(obj, conditional=TRUE)
    * This iterates each variable against correlated variables in a controlled way. Ie the target variable is assessed against subgroups of the data where a correlated variable is constant.
    
For cforest, NA values are not handled. Therefore create a 'core' dataframe consisting of the variables which have very low NA entries (say max 10) and delete all NA rows or impute.

State- impute QLD
Business - 22 NA values - impute 'unknown'
Biz.size - 38 NA's - find median, impute?
Biz.type - 18 NA's impute???
charge.ph - delete
charge.pc - delete
Start.Date - delete
End.Date - delete
no.users - 4 NA values - impute
pc.usersss - 7 NA's - what to do. 0 for all? averages for all?
Num.disc - 84 NA's - impute 1?
num.inv and mean.inv- 1 NA?



```{r core_creation}
#impute NA values in some variables to be in core, call this all7b
all7b<- all7a
all7b[is.na(all7b$State),]$State <- 'QLD'
#RSL Care client - now have details
all7b[all7b$code.client %in% 'C2835',]$no.employees <- 1001
all7b[all7b$code.client %in% 'C2835',]$Biz.type <- 'NFP'
all7b[all7b$code.client %in% 'C2835',]$Business <- 'developer/real estate'
all7b[all7b$code.client %in% 'C2758',]$Biz.size <- 'national'
all7b[all7b$code.client %in% 'C2758',]$no.employees <- 2
all7b[all7b$code.client %in% 'C2758',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2758',]$Business <- 'business'
all7b[all7b$code.client %in% 'C2326',]$Business <- 'health'
all7b[all7b$code.client %in% 'C2338',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2338',]$Business <- 'developer/real estate'
all7b[all7b$code.client %in% 'C2338',]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2882',]$Biz.size <- 'local'
all7b[all7b$code.client %in% 'C2882',]$Business <- 'builder'
all7b[all7b$code.client %in% 'C2882',]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2882',]$no.employees <- 3
#all remaining NA Business will be 'unknown'
all7b$Business<- as.character(all7b$Business)
all7b[is.na(all7b$Business),]$Business <- 'unknown'
all7b$Business<- as.factor(all7b$Business)

#impute Biz.size to be 'local' by default - this is most popular category
all7b[is.na(all7b$Biz.size),]$Biz.size <- 'local'
#impute Biz.type to be 'private' by default - this is most popular category
#client which is 'department of national parks' shall be 'gov'
all7b[is.na(all7b$Biz.type),]$Biz.type <- 'private'
all7b[all7b$code.client %in% 'C2503',]$Biz.type <- 'gov'
#delete charge.ph and charge.pc, Start.Date and End.Date
all7b<- all7b %>% select(-charge.ph, -charge.pc, -Start.Date, -End.Date)
#impute NA no.users as 2 = median and mode
all7b[is.na(all7b$no.users),]$no.users <- 2
#impute NA num.disc as 1 - overwhelmingly the most common
all7b[is.na(all7b$Num.disc),]$Num.disc <- 1
#num.inv and mean.inv both have a single NA value
# this consisted of single large -ve invoice. I think we should delete this case 
all7b<- all7b %>% filter(!is.na(num.inv))
# write.csv(all7b,'all7b.csv')

#create core data frame - variables with very few NA's and delete these NA's

core<- all7b %>% select(code.client , Discipline ,  client.count , inv.mlsto , Business ,
        Biz.size , Biz.type , no.users , Year , timespan , Num.disc , Num.days , mean.peeps,
        pc.contracttech , hours.perday , profit.mlsto , balance.mlsto , hrs.mlsto , 
        cost.mlsto , dis.sc.mlsto , inv.mlsto , return.pdol , code.contact ,
        pc.director , pc.midtech , pc.midpro , pc.gradpro , pc.seniortech , pc.seniorpro ,
        pc.pro , code.director , code.ProjEng , ProjEng.Pos , num.inv , mean.inv , num.neginv , 
        client.meaninv , client.neginv , client.numinv , client.totinv)

var.imp<- cforest(return.pdol~.,
        data= all7b
        )

```


