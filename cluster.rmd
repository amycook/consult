---
title: "clusters"
output: word_document
---

```{r, echo=FALSE, include=FALSE}

library('knitr', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('ggplot2', lib = 'C:/Progra~1/R/R-3.2.1/library')
library("plyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library("dplyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library('magrittr',lib='C:/Progra~1/R/R-3.2.1/library')
library('reshape2',lib='C:/Progra~1/R/R-3.2.1/library')
library("rpart",lib = 'C:/Program Files/R/R-3.2.1/library')
library('car', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('e1071', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('corrgram', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('party', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('randomForest', lib = 'C:/Progra~1/R/R-3.2.1/library')

# library('knitr')
# library('ggplot2')
# library("dplyr")
# library("plyr")
# library('magrittr')
# library('reshape2')
# library("rpart")
# library('car')
# library('e1071')
# library('corrgram')
# library('party')
# library('dendextend')
# 
# setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
# setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
# opts_knit$set(root.dir= "~/OneDrive/shared files/Bligh Tanner/masters/data")

all7c<- read.csv('all7c.csv')[,-1]
all7c$Post.Code<- as.factor(all7c$Post.Code)

```

Cluster exploration!!

time to have a look at if some of these variables with many many categories can be simplified

Remaining categories include:

Full:

inv.mlsto
timespan
no.users
Discipline
pc.pro
client.totinv
code.director
Num.disc
Business
Biz.type

Many NA:

code.contact
code.client
Project.Value
JD.Second
Billing.Type
Post.Code
code.ProjEng
Billing.Type
majority.pos
pc.majpos



Create reduced dataset and have a think about what you want to cluster against each other

```{r final_var, echo=FALSE}

all7d<- all7c %>% select(inv.mlsto, timespan, no.users, Discipline, pc.pro, client.totinv, code.director, Num.disc, Business,
                         Biz.type, code.contact, pc.majpos,
                         code.client, Project.Value, JD.Second, Billing.Type, Post.Code, code.ProjEng, Billing.Type, majority.pos)

```

What kind of clusters are you looking for? Within the numeric variables:

```{r, echo=FALSE}
all7c[,!sapply(all7c, is.factor)] %>% colnames
all7d[,!sapply(all7d, is.factor)] %>% colnames

```


Numeric variables in reduced dataset are: inv.mlsto, timespan, no.users, pc.pro, client.totinv, Num.disc, Project.Value, pc.majpos

Numeric variables worth considering in full dataset: Num.days, mean.peeps, hours.perday, balance.mlsto, hrs.mlsto, cost.mlsto, 

So, lets choose:

* inv.mlsto
* timespan
* no.users
* pc.pro
* client.totinv
* Num.disc
* Num.days
* mean.peeps
* hours.perday
* balance.mlsto
* hrs.mlsto
* cost.mlsto

Many NA - do second
* pc.majpos
* Project.Value

Before clustering, need to standardise variables, and have a look at 'normality' of other variables

```{r var_check, echo=FALSE}

# #Q-Q plot of each variable
# clust<- all7c %>% select(mlsto, inv.mlsto, timespan, no.users, pc.pro, client.totinv,
#                          Num.disc, Num.days, mean.peeps, hours.perday, balance.mlsto, hrs.mlsto,
#                          cost.mlsto, pc.majpos, return.pdol)
# 
# #get rid of zeroes for log transform
# 
# clust[clust$timespan ==0,]$timespan <- 1
# clust[clust$pc.pro ==0,]$pc.pro <- 5
# clust[clust$hours.perday ==0,]$hours.perday <- .1
# clust[clust$hours.perday ==0,]$hours.perday <- .1
# clust[clust$balance.mlsto ==0,]$balance.mlsto <- 1
# clust[clust$hrs.mlsto ==0,]$hrs.mlsto <- 1
# clust[clust$cost.mlsto ==0,]$cost.mlsto <- 1
# clust[clust$return.pdol ==0,]$return.pdol <- .01
# 
# for(i in 1:ncol(clust)){
# qqPlot(clust[,i],
#        main=colnames(clust)[i])
# }
# 
# #qqplots for log of each variable
# for(i in 2:ncol(clust)+1){
# qqPlot(clust[,i] %>% log,
#        main=colnames(clust)[i])
# }
# 
# #qqplots for sqrt of each variable
# for(i in 2:ncol(clust[,-1])+1){
# qqPlot(clust[,i] %>% sqrt,
#        main=colnames(clust)[i])
# }
# 
# #density plots - plain variables
# for(i in 1:ncol(clust)){
# print(
#         ggplot(clust, aes_string(x=colnames(clust[i]))) + geom_histogram(aes(y=..density..))
#         )
# }
# #density plots - log transformed variables
# clust.log<- log(clust)
# for(i in 1:ncol(clust.log)){
#         print(
#                 ggplot(clust.log, aes_string(x=colnames(clust.log[i]))) + geom_histogram(aes(y=..density..))
#                 )
# }
# 
# #plot sqrt of timespan
# clust$timespan <- clust$timespan %>% sqrt
# ggplot(clust, aes_string(x='timespan')) + geom_histogram(aes(y=..density..))

```

taking the log worked for:
* cost.mlsto, hrs.mlsto, hours.perday, Num.days, client.totinv, no.users, inv.mlsto
* sqrt of timespan

so we end up with a numeric data set with the above variables log transformed.
Then scale each by subtracting the mean of each variable and then dividing by the sd of each variable.

```{r clust final, echo=FALSE}
clust<- all7c %>% select(mlsto, inv.mlsto, timespan, no.users, pc.pro, client.totinv,
                         Num.disc, Num.days, mean.peeps, hours.perday, balance.mlsto, hrs.mlsto,
                         cost.mlsto, return.pdol)

#get rid of zeroes for log transform

clust[clust$timespan ==0,]$timespan <- 1
clust[clust$hours.perday ==0,]$hours.perday <- .1
clust[clust$hrs.mlsto ==0,]$hrs.mlsto <- 1
clust[clust$cost.mlsto ==0,]$cost.mlsto <- 1

#take log of all variables to be logged in a for loop
logged<- c('cost.mlsto', 'hrs.mlsto', 'hours.perday', 'Num.days', 'client.totinv',
           'no.users', 'inv.mlsto')
for(i in 1:length(logged)){
        clust[,logged[i]]<- clust[,logged[i]] %>% log
}
clust$timespan<- sqrt(clust$timespan)

#now to standardise
clust<- scale(clust[,-1]) %>% as.data.frame
```

Now to do hierarchical clustering

```{r hierarch, echo=FALSE}

#Ward's clustering

x<- dist(clust, method = 'euclidean')
fit<- hclust(x, method = 'ward.D2')
plot(fit)

rect.hclust(fit, k=5, border='red')
#5 clusters looks pretty good

#try horizontal dend with branches coloured for 8 clusters - very slow
# dend<- fit %>% as.dendrogram
# dend %>% color_branches(k=8) %>% plot(horiz=T)
# rect.dendrogram(dend, k=8, horiz=T)

#how many points in each cluster?
clust$groups<- cutree(fit,k=5)
table(clust$groups)



```

Now look at properties of groups using k means clustering

```{r kmeans, echo=FALSE}

k1<- kmeans(clust %>% select(-groups), 5)
k1.centres<- k1$centers %>% as.data.frame
k1.centres$group<- rownames(k1.centres)
k1.melt<- melt(k1.centres)
k1.melt$group<- k1.melt$group %>% as.factor
ggplot(k1.melt, aes(x=variable, y= value, fill=variable)) + geom_bar(stat='identity', position='dodge')+
        facet_wrap(~group) +
        theme(axis.text.x=element_text(angle=45,hjust=1))

#try plotting means of actual values

all7c$groups<- k1$cluster
kmeans.plot<- all7c %>% select(inv.mlsto, timespan, no.users, pc.pro, client.totinv,
                         Num.disc, Num.days, mean.peeps, hours.perday, balance.mlsto, hrs.mlsto,
                         cost.mlsto, return.pdol, groups)
all7c.plot<- ddply(kmeans.plot, .(groups), numcolwise(mean))
all7c.plot$groups<- rownames(all7c.plot)
k2.melt<- melt(all7c.plot)

#summary means of all numeric variables in 5 clusters
clust.summary<- all7c.plot %>% select(-groups) %>% round(2)
tab<- table(all7c$groups) %>% prop.table %>% as.data.frame
bal<- sum(all7c$balance.mlsto)
hrs<-  sum(all7c$hrs.mlsto)
clust.summary$pc.jobs<- round( tab[,2]*100, 1)
clust.summary$bal.pc<- ddply(all7c, .(groups), summarise, bal.pc= round(sum(balance.mlsto)/bal*100, 1)) %>% 
        select(2)
clust.summary$hrs.pc<- ddply(all7c, .(groups), summarise, hrs.pc= round(sum(hrs.mlsto)/hrs*100, 1)) %>% 
        select(2)

clust.summary<- clust.summary[,c(1,12,10,11, 13, 9,2,7,6,3,4,8,5,14:16)]

# add medians
all7c.med<- ddply(kmeans.plot, .(groups), numcolwise(median))

all7c.med<- all7c.med[, c(2,13,11,12,14,10,3,8,7,4,5,9,6)]


```


Very interesting, now want to optimise number of clusters- refer wards tutorial

```{r optimise clustnum,echo=FALSE}
library('fpc', lib='C:/Progra~1/R/R-3.2.0/library')
library('vegan', lib = 'C:/Progra~1/R/R-3.2.0/library')

#try clustering a low number of numeric variables, trialling iffy ones to see which ones are good for clustering
clust.km<- cascadeKM(clust %>% select(inv.mlsto, balance.mlsto, return.pdol,
                                      hrs.mlsto), 2, 20, iter= 100)
plot(clust.km, sortg = TRUE, grpmts.plot = TRUE)
# client.totinv indicating about 4 clusters
# Num.days - good, indicating 18 custers
# hrs.mlsto - ok, heavy dip, good at about 17 clusters
# pc.pro - 5 groups good, still iffy
# mean.peeps - good at about 4 clusters
# Num.disc - ok, inidicating 12 groups
# timespan: not good for clustering
# no.users: not good for clustering
# hours.perday: no good for clustering

## try out all iffy variables together, eliminate duds
clust.km<- cascadeKM(clust %>% select(inv.mlsto, balance.mlsto, return.pdol,
                                      client.totinv, Num.days, hrs.mlsto, pc.pro, mean.peeps,
                                      Num.disc), 2, 20, iter= 100)
plot(clust.km, sortg = TRUE, grpmts.plot = TRUE)
# all together, no good
# smaller subset of variables I am most interested in that still works for clustering:

clust.km<- cascadeKM(clust %>% select(inv.mlsto, balance.mlsto, return.pdol,
                                      hrs.mlsto
                                      ), 2, 20, iter= 100)
plot(clust.km, sortg = TRUE, grpmts.plot = TRUE)

# 17 clusters
# Cluster cohesion: create plots of SSE index (distance within clusters - sum of squared error) 
# then calinksi (index of cluster homogeneity and distance where higher number is better)
# calinski: between cluster distance/within cluster distance
ind.plot<- melt(clust.km$results)
ggplot(ind.plot %>% filter(Var1 == 'SSE'), aes(x=Var2, y=value, group=1)) + 
        geom_line( ) +
        theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(ind.plot %>% filter(Var1 == 'calinski'), aes(x=Var2, y=value, group=1)) + 
        geom_line( )+
        theme(axis.text.x=element_text(angle=45,hjust=1))
# 17 clusters should be good


```

Analysis shows that inv.mlsto, balance.mlsto, return.pdol, and hrs.mlsto should be used for clustering and that 17 clusters is ideal.
I'm not completely happy with the clustering.
I think I should now add a new variable: inv.mlsto x return.pdol
This will give a 'success' scale, weighing large jobs even with a smaller return.pdol as quite good and tiny jobs with a good return.pdol as average to low.

this means I will need to check normality, scale the variable before adding it to clust

```{r new var, echo=FALSE}
#new variable for all7c - success
# success= return.pdolxinv.mlsto

all7c<- transform(all7c, suc= inv.mlsto*return.pdol)
all7c$suc %>% summary

all7c %>% arrange(-suc) %>% head

qqPlot(all7c$return.pdol,
       main='success')

qqPlot(sapply(all7c$suc, function(x){sign(x)*log(abs(x) + 1)}))
qqPlot((all7c$suc) %>% math.cbrt)

#can't log because of negatives

#density plots - plain variables

ggplot(all7c, aes_string(x='inv.vs.cost')) + geom_histogram(aes(y=..density..))

#density plots - log transformed variables

trial<- (sapply(all7c$suc, function(x){sign(x)*log(abs(x) + 1)})) %>% as.data.frame
colnames(trial)[names(trial) %in% '.']<-'var'
ggplot(trial, aes_string(x='var')) + geom_histogram(aes(y=..density..))
skewness(trial$var)
kurtosis(trial$var)
mean(trial$var)
sd(trial$var)
trial$scale.var<- scale(trial$var)
ggplot(trial, aes_string(x='scale.var')) + geom_histogram(aes(y=..density..))
# ggplot still looks bimodal after scaling

## conclusion - lets take neglog of suc and scale it, add it to clust

clust$suc<- sapply(all7c$suc, function(x){sign(x)*log(abs(x) + 1)}) %>% scale()

## should we make return.pdol bimodal??
math.cbrt <- function(x) {
        sign(x) * abs(x)^(1/3)
}
qqPlot(all7c$return.pdol %>% math.cbrt,
       main='success')
trial<- (all7c$return.pdol %>% math.cbrt) %>% as.data.frame
colnames(trial)[names(trial) %in% '.']<-'var'
ggplot(trial, aes_string(x='var')) + geom_histogram(aes(y=..density..))
skewness(trial$var)
kurtosis(trial$var)
trial$scale.var<- scale(trial$var)
ggplot(trial, aes_string(x='scale.var')) + geom_histogram(aes(y=..density..))
#take cube root of return.pdol and scale it to clust

clust$return.pdol<- all7c$return.pdol %>% math.cbrt %>% scale()

```


Repeat clustering with new suc and revised bi-modal return.pdol

```{r new successvar, echo=FALSE}

#investigate new variable a little

clust.km<- cascadeKM(clust %>% select(inv.mlsto, balance.mlsto, return.pdol,
                                      suc, hrs.mlsto), 2, 15, iter= 100)
plot(clust.km, sortg = TRUE, grpmts.plot = TRUE)

# 10 clusters
# Cluster cohesion: create plots of SSE index (distance within clusters - sum of squared error) 
# then calinksi (index of cluster homogeneity and distance where higher number is better)
# calinski: between cluster distance/within cluster distance
ind.plot<- melt(clust2.km$results)
ggplot(ind.plot %>% filter(Var1 == 'SSE'), aes(x=Var2, y=value, group=1)) + 
        geom_line( ) +
        theme(axis.text.x=element_text(angle=45,hjust=1))
ggplot(ind.plot %>% filter(Var1 == 'calinski'), aes(x=Var2, y=value, group=1)) + 
        geom_line( )+
        theme(axis.text.x=element_text(angle=45,hjust=1))

# 10 clusters should be good

# have a look at dendrogram with 10 clusters

x<- dist(clust %>% select(inv.mlsto, balance.mlsto, return.pdol, suc, hrs.mlsto), 
         method = 'euclidean')
fit<- hclust(x, method = 'ward.D2')
plot(fit)

rect.hclust(fit, k=10, border='red')

#looks good!



```

now want 10 clusters, based on k means optimisation, have a look at average characteristics

```{r kmeans, echo=FALSE}

k1<- kmeans(clust %>% select(inv.mlsto, balance.mlsto, return.pdol, suc, hrs.mlsto), 10)

#try plotting means of actual values

all7c$groups<- k1$cluster
kmeans.plot<- all7c %>% select(inv.mlsto, timespan, no.users, pc.pro, client.totinv,
                         Num.disc, Num.days, mean.peeps, hours.perday, balance.mlsto, hrs.mlsto,
                         cost.mlsto, return.pdol, suc, groups)
all7c.plot<- ddply(kmeans.plot, .(groups), numcolwise(mean))
all7c.plot$groups<- rownames(all7c.plot)
k2.melt<- melt(all7c.plot)

#summary means of all numeric variables in 5 clusters
clust.summary<- all7c.plot %>% select(-groups) %>% round(2)
tab<- table(all7c$groups) %>% prop.table %>% as.data.frame
bal<- sum(all7c$balance.mlsto)
hrs<-  sum(all7c$hrs.mlsto)
clust.summary$pc.jobs<- round( tab[,2]*100, 1)
clust.summary$bal.pc<- ddply(all7c, .(groups), summarise, bal.pc= round(sum(balance.mlsto)/bal*100, 1)) %>% 
        select(2)
clust.summary$hrs.pc<- ddply(all7c, .(groups), summarise, hrs.pc= round(sum(hrs.mlsto)/hrs*100, 1)) %>% 
        select(2)
clust.summary$job.count<- table(all7c$groups) %>% as.data.frame %>% select(Freq)

clust.summary<- clust.summary %>% select(inv.mlsto, suc, return.pdol, cost.mlsto, balance.mlsto, hrs.mlsto, hours.perday, timespan, Num.days, Num.disc, no.users, pc.pro, mean.peeps, client.totinv, pc.jobs, bal.pc, hrs.pc)


# add medians
all7c.med<- ddply(kmeans.plot, .(groups), numcolwise(median))

all7c.med<- all7c.med %>% select(inv.mlsto, suc, return.pdol, cost.mlsto, balance.mlsto, hrs.mlsto, hours.perday, timespan, Num.days, Num.disc, no.users, pc.pro, mean.peeps, client.totinv)


```



