---
title: "Regression"
author: "Amy Cook"
date: "August 15, 2015"
output: word_document
---

```{r, echo=FALSE, include=FALSE}

library('knitr', lib = 'C:/Progra~1/R/R-3.2.1/library')
library('ggplot2', lib = 'C:/Progra~1/R/R-3.2.1/library')
library("plyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library("dplyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library('magrittr',lib='C:/Progra~1/R/R-3.2.1/library')
library('reshape2',lib='C:/Progra~1/R/R-3.2.1/library')
library('caret',lib='C:/Progra~1/R/R-3.2.1/library')
# library("rpart",lib = 'C:/Program Files/R/R-3.2.1/library')
library('car', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library('e1071', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library('corrgram', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library('party', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library('randomForest', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library('fpc', lib='C:/Progra~1/R/R-3.2.1/library')
# library('vegan', lib = 'C:/Progra~1/R/R-3.2.1/library')
# library(devtools,lib='C:/Progra~1/R/R-3.2.1/library')


library(devtools)
library('knitr')
library('ggplot2')
library("plyr")
library("dplyr")
library('magrittr')
library('reshape2')
library("rpart")
library('car')
library('e1071')
library('corrgram')
library('randomForest')
library('caret')


# 
# setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
# setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
opts_knit$set(root.dir= "~/OneDrive/shared files/Bligh Tanner/masters/data")
# opts_knit$set(root.dir= "C:/Users/n9232371/Documents/Consultbusiness/data")
all7d<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all7d.csv')[,-1]
all7d<- read.csv('~/OneDrive/shared files/Bligh Tanner/masters/data/all7d.csv')[,-1]
all7d$Post.Code<- as.factor(all7d$Post.Code)

```


This file is to do a multi-step regression to predict RETURN PER DOLLAR.
The reason for multistepping is to allow for multiple variables with lots of missing NA values.

Variables to include:

Full:

inv.mlsto
timespan
no.users
Discipline
pc.pro
client.totinv
code.director
Num.disc
Business
Biz.type
code.ProjEng

Many NA:

code.contact
code.client
Project.Value
JD.Second
Post.Code
Billing.Type
majority.pos
pc.majpos

The 'Many NA' variables will be split into groups:

* code.contact: 1652 NA
* code.client: 422 NA
* Project.Value: 2333 NA
* JD.Second: 1057 NA
* Post.Code: 356 NA
* Billing.Type: 1314 NA
* majority.pos: 377 NA
* pc.majpos: 377 NA

Will address each of the variables with many NA separately. 8 of these variables totally. run a separate lm for each of these

No need to transform numeric variables - not required for trees

Root mean square error as initial check.

```{r, echo=FALSE}

#predict Petal.Width using lm, find RMSE
#answer will be return.pdol
#guess will be response of prediction which is cube root
RMSE<- function(answer, guess){
        error= answer-guess
        root.error= sqrt(mean(error^2))
        print(root.error)
}

```

Now fit full variables to random forest, tune mtry and number of trees

Try rpart first

```{r, echo=FALSE}

train_ind <- sample(seq_len(nrow(all7d)), size = nrow(all7d)*2/3)

train <- all7d[train_ind, ]
test <- all7d[-train_ind, ]

#create tree with training data only
tree.all7d<- rpart(return.pdol ~ inv.mlsto + timespan + no.users + Discipline + pc.pro + 
                     client.totinv + code.director + 
                  Num.disc + Business + Biz.type + code.ProjEng, data=train, control = rpart.control(cp= 0.001))
summary(tree.all7d)

#have a look at cp values, automatic cross validation in rpart
printcp(tree.all7d)
plotcp(tree.all7d)
#min cp = 0.013 will give simplest tree within 1 std of x-val rel error
tree.all7d<- rpart(return.pdol ~ inv.mlsto + timespan + no.users + Discipline + pc.pro + 
                     client.totinv + code.director + 
                  Num.disc + Business + Biz.type + code.ProjEng, data=train, control = rpart.control(cp= 0.011))


split.fun <- function(x, labs, digits, varlen, faclen){
  # replace commas with spaces (needed for strwrap)
  labs <- gsub(",", " ", labs)
  for(i in 1:length(labs)) {
    # split labs[i] into multiple lines
    labs[i] <- paste(strwrap(labs[i], width=25), collapse="\n")
    }
  labs
  }

prp(tree.all7d, extra = 1, split.fun= split.fun, tweak=1, faclen =5, branch.type= 5)
#make prediction with current tree
tree.pred<-predict(tree.all7d, test)
#calculate RMSE for test data
RMSE(test$return.pdol, tree.pred)
# 51.4 cents, already close to regression..


#try to make a rainbow tree
heat.tree <- function(tree, low.is.green=FALSE, ...) { # dots args passed to prp
  y <- tree$frame$yval
  if(low.is.green)
    y <- -y
  max <- max(y)
  min <- min(y)
  cols <- rainbow(99, end=.36)[
    ifelse(y > y[1], (y-y[1]) * (99-50) / (max-y[1]) + 50,
           (y-min) * (50-1) / (y[1]-min) + 1)]
  prp(tree, branch.col=cols, box.col=cols, ...)
  }

heat.tree(tree.all7d, extra = 1, split.fun= split.fun, tweak=1, faclen =5, branch.type= 5)


```


RMSE is 0.514 which is worse than regression in predicting return per dollar.

Decision trees not affected by too many variables - leave all in

Decision trees cannot handle pairwise interactions

Let's try 10 fold cross validation and have a look at the error.
Use caret package

```{r, echo=FALSE}
#10 fold cross validation using caret package
#specify 10 fold
tc <- trainControl("cv",10)
#keep cp static at 0.011
rpart.grid <- expand.grid(.cp=0.011)
train.rpart <- train( return.pdol ~ inv.mlsto + timespan + no.users + Discipline + pc.pro + 
                     client.totinv + code.director + 
                  Num.disc + Business + Biz.type + code.ProjEng, data=all7d, method="rpart",trControl=tc,
                  tuneGrid=rpart.grid)

#RMSE = 0.50386
# Rsquared = 0.0844, quite bad

```


Let's try adding more variables:

```{r, echo=FALSE}
#try adding code.client

addone<- function(var= 'JD.Second', df= all7d){
        reduced = df[!(is.na(df[,var])),]
        formula = paste("return.pdol ~ inv.mlsto + timespan + no.users + Discipline + pc.pro + 
                     client.totinv + code.director + 
                  Num.disc + Business + Biz.type + code.ProjEng", var, sep=" + ")
        add.one = rpart(as.formula(formula), data = reduced)
        
        #plot decision tree
        print(
          heat.tree(add.one, extra = 1, split.fun= split.fun, tweak=1, faclen =5, branch.type= 5)
          )
        
        #perform 10 fold xval using caret package
        tc = trainControl("cv",10)
        #keep cp static at 0.011
        rpart.grid = expand.grid(.cp=0.011)
        train.rpart = train(as.formula(formula), data=df, method="rpart",trControl=tc,
                  tuneGrid=rpart.grid)
        
        
        print(
        train.rpart$results
        )
        
}

#without additional variables, RMSE sits at 0.4759

addone(var='JD.Second', df=all7d)
#RMSE = .4709
addone(var='Post.Code', df=all7d)
#RMSE = .4353
addone(var='majority.pos', df=all7d)
#RMSE = .4541
addone(var='pc.majpos.log', df=all7d)
#RMSE = .4553
addone(var='code.client', df=all7d)
#RMSE = .4394
addone(var='Billing.Type', df=all7d)
#RMSE = .4579
addone(var='code.contact', df=all7d)
#RMSE = .3972

#dont use post code ever. 
#JD.Second doesn't help that much. acutally makes RMSE worse.

```

Delete Post.Code option

Billing.Type should be paired with inv.mlsto and maybe Discipline

maybe client should be paired with inv.mlsto
maybe JD.Second should be paired with inv.mlsto and code.client and timespan and pc.pro and no.users?? which ever one does something.

majority.pos pair with inv.mlsto
pc.majpos pair with inv.mlsto

Write a function that lists the extra variable you're trying and write out the interaction.


```{r, echo=FALSE}

addmult<- function(var= c('JD.Second', 'code.client'), combos= c('JD.Second*inv.mlsto.log'), df= all7d){
        reduced = df[complete.cases(df[,var]),]
        formula = paste("return.pdol ~ inv.mlsto.log + timespan.cbrt + no.users + Discipline + pc.pro + inv.mlsto.log*pc.pro + inv.mlsto.log*Business")
        
        for(i in 1:length(var)){
                formula = paste(formula, var[i], sep= " + ")
        }
        
        for(i in 1:length(combos)){
                formula = paste(formula, combos[i], sep= " + ")
        }

        add.one = lm(as.formula(formula), data = reduced)
        
        print(summary(add.one))
        
        ans= data.frame(answer= reduced$return.pdol, addone.lm = predict(add.one, reduced, interval= 'predict'))
        
        print(head(ans))
        
        print(
        paste("RMSE=", RMSE(ans$answer, ans$addone.lm.fit) %>% round(4), sep= " ")
        )
        
}

#try Billing.Type with inv.mlsto.log
addmult(var= c('Billing.Type'), combos= c('Billing.Type*inv.mlsto.log'), df= all7d)
# no

#try code.client*inv.mlsto.log
addmult(var= c('code.client'), combos= c('code.client*inv.mlsto.log'), df= all7d)
#made a difference, brought RMSE down to .4076 :)!

#try JD.Second with inv.mlsto.log (no), code.client (too intense), timespan (no), pc.pro (no), no.users (yes)?
addmult(var= c('JD.Second'), combos= c('JD.Second*no.users'), df= all7d)
#add JD.Second*no.users

addmult(var= c('majority.pos'), combos= c('majority.pos*inv.mlsto.log'), df= all7d)
#no

addmult(var= c('pc.majpos.log'), combos= c('pc.majpos.log*inv.mlsto.log'), df= all7d)
#no


```

It is worthwhile to include:

* code.client by inv.mlsto.log because brought down RMSE from 0.4394 (code.client) to 0.4076 (code.client + code.client by inv.mlsto.log)
* JD.Second by no.users. Brought down RMSE from 0.4709 to 0.4612...


Now look at cross validation for full variable set:

```{r, echo=FALSE}

#using caret package
tc <- trainControl("cv",10,savePred=F)
fit<- train(return.pdol ~ inv.mlsto.log + Discipline + timespan.cbrt + no.users + pc.pro + Business + 
                      inv.mlsto.log*pc.pro + inv.mlsto.log*Business,
              data=all7d,
            method = 'lm', trControl = tc)

#RMSE = 0.49246
#compared to non-cross validated
# RMSE = 0.4759
```

Not really trying to tune anything via cross validation, however RMSE increased from 0.4759 to 0.49246.

Not THAT bad.

Now, how to get variable importance from lm..

```{r, echo=FALSE}

#variable importance for full variables only - pairs.lm
#use varImp from caret package - uses t-test statistic

imp<- varImp(pairs.lm, scale=FALSE)
imp$variable<- rownames(imp)
imp<- imp %>% select(variable, Overall)
imp %>% arrange(-Overall) %>% head()

sum(imp$Overall)

```


Trial some scenarios:

Run full variables plus

Make a function to output:

* var importances
* RMSE

based on a list of variables that you want to use

```{r, echo=FALSE}

sample<- all7d[1000,]
sample<- sample[,!is.na(sample)]
vars<- names(sample) 
vars<- vars[! vars %in% c('mlsto','Year','inv.mlsto','timespan','client.totinv','code.director','Num.disc','Biz.type','Post.Code',
               'code.ProjEng','return.pdol','cost.mlsto','hrs.mlsto','balance.mlsto','client.totinv.log','pc.majpos')]
vars


RMSE<- function(answer, guess){
        error= answer-guess
        root.error= sqrt(mean(error^2))
        # print(root.error)
}


final.output<- function(df=all7d, samp = 800) {
        #create sample df
        sample = df[samp,]
        sample = sample[,!is.na(sample)]
        vars= names(sample)
        vars = vars[! vars %in% c('mlsto','Year','inv.mlsto','timespan','client.totinv','code.director','Num.disc',
                                  'Biz.type','Post.Code','code.ProjEng','return.pdol','cost.mlsto','hrs.mlsto',
                                  'balance.mlsto','client.totinv.log','pc.majpos')]
        variables = vars
        
        print(
                list("sample case", t(sample %>% select(-Year, -timespan.cbrt, -inv.mlsto.log, -client.totinv.log, -pc.majpos.log)))
        )
        
        #make reduced dataset with complete cases for all variables we want to use
        reduced = df[complete.cases(df[,variables]),]
        
        print(
                paste(nrow(reduced), "cases in full model")
        )
        
        formula = paste("return.pdol ~ ")
        
        for(i in 1:length(variables)){
                formula = paste(formula, variables[i], sep= " + ")
        }
        
        if('inv.mlsto.log' %in% variables &
           'inv.mlsto.log' %in% variables){
                formula = paste(formula, 'inv.mlsto.log*pc.pro', sep= " + ")
        }
        
        if('inv.mlsto.log' %in% variables &
           'code.client' %in% variables){
                formula = paste(formula, 'inv.mlsto.log*code.client', sep= " + ")
        }
        
        if('JD.Second' %in% variables &
           'no.users' %in% variables){
                formula = paste(formula, 'JD.Second*no.users', sep= " + ")
        }

        final = aov(as.formula(formula), data = reduced)
        
        fit= summary(final)
        
        ans= data.frame(answer= reduced$return.pdol, final.lm = predict(final, reduced, interval= 'predict'))
        
        # print(head(ans))

        
        #print variable importances for reduced lm just run
        
        imp= data.frame('vars'= rownames(fit[[1]]), 'F.val' = fit[[1]]$`F value`)
        #remove white space from levels in imp$vars
        levels(imp$vars)<- sub("\\s+$", "", levels(imp$vars))
        #remove Residuals row
        imp= imp[!(imp$vars %in% 'Residuals'),]
        
        #print variable importances
        print(
                list("variable importances",
                     imp %>% arrange(-F.val) %>% head())
        )
        
        
        #print RMSE for cases that match certain categories only
        #first subset predictions to cases that match the category
        
        reduced$predict= ans$final.lm.fit
        
        cats= c('code.client', 'code.contact', 'JD.Second', 'Business', 'Discipline', 'majority.pos')
        
        for(i in 1:length(cats)){
                if(cats[i] %in% vars){
                        cats.red = reduced[reduced[,cats[i]] %in% sample[,cats[i]],]
                        cat.RMSE = RMSE(cats.red$return.pdol, cats.red$predict) %>% round(4)
                        print(
                                paste(cats[i], "RMSE =", cat.RMSE, sep= " "))
                        print(
                                paste("Number of cases =", nrow(cats.red), sep= " "))
                        print(
                                head(cats.red[,c('mlsto','inv.mlsto','cost.mlsto','return.pdol','predict',cats[i])])
                        )
                        }
        }
        
        final.RMSE= RMSE(ans$answer, ans$final.lm.fit) %>% round(4)
        
        print(
        paste("Overall predicted RMSE=", final.RMSE , sep= " ")
        )
        
        predict.rpdol = predict(final, sample) %>% round(3)
        predict.rpdol = predict.rpdol[[1]]
        predict.cost = sample$inv.mlsto/(1+predict.rpdol) %>% round(1)
        print(
               paste("return per dollar prediction", predict.rpdol, sep = " ")
        )
        
        print(
               paste("answer", sample$return.pdol %>% round(3), sep = " ")
        )
        
        print(
                paste("Invoiced= $", sample$inv.mlsto, ",   predicted cost= $", predict.cost %>% round(1), sep="")
        )
        
        print(
                paste("Actual cost= $", sample$cost.mlsto, sep="")
        )

        
}

final.output(df=all7d, samp = 500)

```












