---
title: Predictive Models to Support Quoting of Fixed Fee Consulting Projects
author:
  - name: Amy Cook, Paul Wu, Kerrie Mengersen
    email: a21.cook@qut.edu.au
    affiliation: Queensland University of Technology
    footnote: Corresponding Author
address:
  - code: Queensland University of Technology
    address: School of Mathematical Sciences, George Street, Brisbane, QLD, 4000
abstract: |
  A concise abstract is required. limit to 250 words. clearly state purpose of research, principal resutls and major conclusions. no refs
  
  Engaging in loss making jobs for fixed fees is a major problem in consulting, particularly in the competitive construction industry. This thesis investigates whether machine learning techniques applied to a company's passively collected internal data could help avoid loss making jobs or help tactfully choose when to enforce stricter contracts. It was found that in a specific decision framework, a case study's profits could be improved 9% by declining approximately 4% of projects. Alternative decision frameworks are also proposed and evaluated. Algorithmic methods such as Logistic Regression, Random Forests, Boosted Trees, Naive Bayes, and Bayesian Networks were applied as well as blended combinations of these methods. A decision scenario which rejected projects above a sequence of tested thresholds was run in order to find the optimal threshold for profit improvements. The blended Logistic Regression model outperformed other methods and produced a 95% confidence interval of 6.5 - 11.5% profit improvements. The findings from this research have the potential to assist managers in reducing losses by highlighting risky projects and guiding project-based changes to fee structures.
keywords: |
  consulting; machine learning; profitability; predictive model; construction industry; data mining
abbreviations: 
  - name: Customer Relationship Management
    footnote: Abbreviations

bibliography: Library.bib
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_

```{r, echo=FALSE, include=FALSE}

library('knitr')
library('ggplot2')
library("plyr")
library("dplyr")
library('magrittr')
library('reshape2')
library("rpart")
library('car')
library('e1071')
library('randomForest')
library('RColorBrewer')
library('pwr')
library('scales')
library('purrr')


# detect OS
if(.Platform$OS.type == 'windows'){
        # setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
        opts_knit$set(root.dir= "C:/Users/n9232371/Documents/github/consult/finalwriteup/report_data")
} else{
        # setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
        # setwd("~/Documents/github/consult/finalwriteup/report_data")
        opts_knit$set(root.dir= '~/Documents/github/consult/finalwriteup/report_data')
}

opts_chunk$set(fig.width=6.7, dpi = 200, warning = FALSE, message = FALSE, echo = FALSE, warning = FALSE)
```

1. Introduction
==========================

Clearly state the research question and objectives of the work. Briefly provide any necessary
background to frame the research question. Concisely summarize the major findings/results.

Many consulting projects can be financialy risky endeavors for the consultant, often depending on the industry. Engineering consultanting companies in the construction industry complete a wide range of complex projects in a competitive enviroment. Because of the way contracts are structured, many of these projects result in losses. The purpose of this study was to test whether these loss making projects could be predicted by statistical and machine learning algorithms using the information available at project conception. Secondly, could the predictive algorithm improve a company's profitability? Predictive models and value frameworks were built using project data from a case study engineering company to answer these questions.    

1 page


#### 1.1 Problem motivation

Financial risk is taken on by a consulting company when they offer a fixed fee to complete a project. In competitive climates, clients are in the position to request fixed price quotes from several consultants before a project is awarded. Hence, initial quoted fees must be competitive but also accurately cover consultant salaries and business costs. 

How do consultants make this accurate yet competitive fee estimate? This varies from industry to industry, but typically a consulting manager has experience in the type of project they are quoting and, after reviewing the project details, can use a combination of intuition, analysis of project details, and rules of thumb. Unfortunately, some projects priced using this method can blow out of budget, causing significant financial losses to the consultant.

Cost blowouts of complex projects has a history, particularly documented in the It and construction industry. A review of 6 IT project surveys between 1984 and 1994 by Moløkken & Jørgensen (2003) revealed that 60-80% of IT projects encountered effort and/or schedule overruns, where the average overrun was by 30 to 40%.
A study on large-scale infrastructure projects over the past seventy years revealed that cost forecasts consistently underestimated the cost of rail projects by an average of 44.7%, the cost of bridge and tunnel projects by 33.8% and the cost of road projects by 20.4% [@Flyvbjerg2007].

Kahneman won a nobel prize in 2002 for his research into decision making on this topic. He argued that a person's natural optimistic view of their own skills leads to consistent underestimation of the time and risks involved in a project [@Lovallo2003]. To counter this phenomenon, research by Flyvbjerg [-Flyvbjerg2011] advised that managers should be confronted by the financial performance of similar projects to the one being costed so that they take on a more realistic view of their abilities. This was based off a body of findings where people made better predictions about themselves after being exposed to facts about other people. For example, subjects were asked to predict their skill levels before and after being exposed to a summary of other people's skills, and their predictions after were significantly more accurate [@Lovallo2003].

This work aims to help managers provide better fixed price quotes by predicting whether a new project will be profitable or not, based on an algorithm trained on all previous project data from the same company. The manager could take multiple actions based on the algorithms output including not tendering for a project predicted to be loss-making, providing a high fixed price, or restructuring the contract so that it is not 100% fixed price. This research tests the simple case of rejecting all projects predicted as loss-making.


#### 1.2 Case Study

Project data from a single case study consulting company in the construction industry was used to build the predictive algorithm, trial several methods, and test their impact on the company's bottom line. Twelve years of passively collected data that described each project in terms of their clients, invoice history, employee hours and technical details was scraped from the company's internal database. Customer relationship management (CRM) software provided accesses to the database to all employees. Employees completed daily timesheets, alotting their hours to certian projects. Project managers input technical information and client details for each project, while the finance team tracked invoicing and additional project costs such as printing and transport in the same system. On top of this information, the CRM had the hourly cost of each level of employee. The cleaned dataset consisted of 2364 projects between 2004 and 2012. 70% of projects were fixed price and 20% of all projects made a loss after taking into account business costs.

Once several methods were built, to assess whether the model would improve the case study company's profitability, a simple managerial decision framework was applied: if the algorithm predicted the project to be loss making, the project was considered 'rejected' and all profits or *losses* of rejected projects were discarded. The remaining projects' profits and losses were summed to give a revised overall profit over the last 12 years of projects. This was compared to the summed profits and losses from all projects completed over the last 12 years. The best algorithm rejected enough loss-making projects to improve overall profits by 9%.

<!-- **General Aim** Use statistical techniques to model the profitability of projects for consulting businesses using their internal CRM data. Research will focus on a case study Engineering consulting company that offers their expert advice (in the currency of time) to business clients.  -->
<!-- The project outcomes are intended to assist the business in predicting project profitability before engagement. Several statistical and machine learning techniques were tested, compared and refined. -->

<!-- **Hypothesis 1** -->
<!-- A statistical or machine learning model based on historical project data can predict the profitability of a new project with greater accuracy than a baseline predictor. A baseline predictor for this project is one that predicts the average of a numeric response variable for all cases or, if the response variable is categorical, randomly assigned categories for each case, where the proportion of assigned categories matches the true categorical proportions. -->

<!-- **Hypothesis 2** -->
<!-- The predictive model built from Hypothesis 1 can be shown to have a positive impact on the overall profit earned by the case study business. The overall profit is represented by the following equation: -->


2. Literature Review
====================

<!-- Summary of Key Related Research  -->
<!-- This section should include a brief summary of key related research. Emphasis should be on demonstrating the foundation for the current investigation. Specifically, the goal is to  -->
<!-- clearly delineate a gap or missing link that the current  -->
<!-- research fills. Authors should avoid presenting a litany of past research and should focus -->
<!-- on prior work necessary to demonstrate the -->
<!-- existence of the research gap addressed in the manuscript. -->

For the case study company, training the algorithm to predict actual fee was not possible because the projects were too varied, and there was inadequate information describing the size and scope of each project in the database. However, in the literature, similar studies typically did attempmt to predict project costs of IT consutling projects and final construction costs of buildings and infrastructure.

#### 2.1 Cost estimation in the Construction Industry and IT Industry

Previous studies in the literature have focussed on reporting the predictive accuracy of their cost estimation models for IT or construction projects using data from multiple businesses or governments around the world. In contrast, this research is focussed on determining how or if models predicting project profitability can improve the bottom line for a case study business, given data the company has internally generated. 

Reviewing a range of individual studies show that cost estimation models have generally been built from at most 530 projects [@Kim2004] [@Finnie1997] - 299 software development[@Pai2013] - 163 software [@Shin2015] - 234 building projects, and often under 100 projects [@Finnie1997] [@Chan2005] 87 buildings. Sometimes it is not stated where the project data came from, and some studies specified the information came from surveys, with responses from companies and governments around the world. In this study, the dataset originates from a single company's database, which means the data is easily accessible to the company and there are 2364 projects, which is much more than previous studies. A study by @Mendes2004 demonstrated that using within company data was also more accurate than across company data. Their dataset consisted of 67 across company web projects and 14 within company projects. 

Almost all past studies analysed their model performance by predictive accuracy (reported as R^2 or RMSE) of cost predictions [@llist some stff] and compared these statistics bewteen methods. However, these results were not translated into a business case for the impact these models could have on an institution or how the model could be practically integrated into decision making. For example, if an institution used the model, how would it be applied to their decision making process and what magnitude of benefits could they expect? Saradhi & Palshikar's [-@Saradhi2011] work on employee churn, where 'churn' refers to the number of individuals moving out of a group within a certain time, presented a good framework for implementing their model. Theirsupport vector machine (SVM) predictive model identified employees at high risk of churn. Then a method for determining the value of each employee in terms of their importance to projects and their monthly chargeability was described, which enabled high risk employees to be ranked. Managers then had a clear direction for who to target first to prevent churn. This extension provided a comprehensive framework for how business managers could adopt a machine learning algorithm to improve business operations. It was a valuable addition that is absent from most cost estimation research, but will be included in this work.

#### 2.2 Methods used in Cost Estimation and Other Business Applications

Cost estimation literature tended to apply linear regression, neural networks and sometimes SVM methods, while predictive methods applied to other business problems investigated a wider range. This study aims to compare a wider range of methods to past cost estimation studies.

In the construction industry, often only one method was assessed, such as linear regression, without comparison to other methods [@Chan2005; @Elfaki2014]. Neural Networks started appearing in literature in the 1990's and @Elfaki2014's review of cost estimation methods from 2004 to 2014 found that artificial Neural Networks and SVM's were the most popular machine learning techniques, but studies tended to only test one method. Some studies showed that Neural Networks outperformed Linear Regression, however other studies established they are approximately equal [@Kim2004; @Attalla2003]. To the authors' knowledge @Shin2015 documented the only application of ensemble tree methods (Boosted Trees) to cost estimation in construction projects. They found Boosted Trees slightly outperformed Neural Networks, but not significanatly.

In the software cost estimation literature, Linear Regression was the most popular method, however multiple studies showed that Neural Networks definitively outperform regression models  [@Finnie1997; @Pai2013; @Matson1993]. Despite the higher performance, Neural Networks were criticised for not providing reasoning or structure behind their predictions (black-box predictor) [@Finnie1997].

The review of methods applied to the construction and IT industry highlighted the use of Linear Regression, Neural Networks, SVM's and in one case Boosted Trees, however research on other business problems utilised a wider range of methods. In financial credit scoring, a study by @Brown2012 found that Random Forests and Boosted Trees consistently outperformed Neural Networks in classification. In addition, ensemble trees such as Random Forests and Boosted Trees can output partial dependency plots and variable importance measures that provide insight into the model's calculations. This is a valuable asset oer black box predictors. @Kumar2007 performed a detailed review of statistical and machine learning techniques applied over 37 years in the context of bankruptcy prediction in banks and reported that although SVM's performed well, they are often complex and slow, requiring a great deal of memory. They also assessed method blending techniques, which refers to combinations of two completely different algorithms, and found they can often outperform individual methods such as Linear Regression or Boosted Trees alone.

To date, Linear Regression, Neural Networks, and Support Vector Machines (SVM's) have been comprehensively applied to cost estimation. This research does not test black-box predictors (Neural Networks and SVM's) because insight into the model's predictions is crucial for facilitating user uptake in the case study company. Ensemble tree methods such as Random Forests and Boosted Trees will be used instead, as they can perform as well as Neural Networks while providing insight to their output [@Caruana2006]. Blending of multiple predictive models is also applied in this work and has not yet been done in cost estimation. 

#### 2.3 Gaps

In comparison to previous studies, this project advances the body of work on cost estimation in two ways. First, it focusses on building a model that is practical and profitable for a business to implement by using the case study's interenal database as well as testing a managerial decision framework based off the model output. This framework presents a clear measure of expected benefits provided by adopting the profitability model into a business' decision process. Secondly, advanced predictive methods are trialled which have been minimally tested on this problem. These include ensemble decision tree methods, such as Random Forests and Boosted Trees, and blends of multiple models built from different methods.

 

3. Prediction Methods
=====================

Should provide sufficient detail to allow the work to be reproduced. Methods already published should be indicated by a reference: only relevant method modifications should be described.

A range of simple and complex, linear and non-linear methods were applied to the problem including Logistic Regression, Random Forests, Gradient Boosted Trees, and Naiive Bayes. Furthermore, logistic regression, random forests, boosted trees and simple averaging were used to blend predictions from individual models. A brief description of each method is provided below.

<!-- Neural Netowrks and SVM were not purusued because black box -->
<!-- A previously mentioned disadvantage is that the NN algorithm is a black box because the internal structure is too complex for interpretation. They also require a lot of training data relative to other methods. -->

<!-- Also, SVM's can be very slow to train and therefore not suitable for industry purposes [@Auria2008]. Because it is so important to engage decision makers with a model that can explain its results, these two methods are not appropriate for the effort estimation problem.  -->

#### 3.1 Predictive methods

The aim of this research is to build a model that predicts whether a consulting project will be profitable or not. Therefore each method must be capable of binary classifcation. Binary classification output from a model is the probability, between 0 and 1, of an event occuring. In this case, the event is a project being loss-making.

##### 3.1.1 Logistic Regression

Logisitic regression models a linear relationship bewteen the log odds of a binary response variable and each of the explanatory variables. This is performed similarly to linear regression, but is transformed to probability of an event occuring by taking the inverse log of the log odds and rearranging [@Macdonald1975]:

$$
\begin{aligned}
\log(odds) &= \beta_0 + X_1\beta_1 \\
\log\left(\frac{p}{1-p}\right) &= \beta_0 + X_1\beta_1 \\
-\log\left(\frac{1-p}{p}\right) &= \beta_0 + X_1\beta_1 \\
-\log\left(\frac{1}{p} - 1\right) &= \beta_0 + X_1\beta_1 \\
\log\left(\frac{1}{p} - 1\right) &= -\beta_0 - X_1\beta_1 \\
\exp\left(\log\left(\frac{1}{p} - 1\right)\right) &= \exp(-\beta_0 - X_1\beta_1) \\
\frac{1}{p} - 1 &= \exp(-\beta_0 - X_1\beta_1) \\
\frac{1}{p} &= 1 + \exp(-\beta_0 - X_1\beta_1) \\
p &= \frac{1}{1 + \exp(-\beta_0 - X_1\beta_1)}
\end{aligned}
$$

The result in a sigmoidal function bound by 0 and 1. A coefficient represents the change in the log odds of the response variable for each unit increase in an explanatory variable. Logistic Regression is a good  benchmark to compare other binary predictive models due to its simplicity and speed [@Moore1989]. Addittionally, because of the simple, single linear relationship, the method is not prone to overfitting [@Perlich2003].

##### 3.1.2 Random Forests

Ensemble tree methods were pioneered in the 1990's to combat the low predictive accuracy and instability of individual decision trees. Combining hundreds or thousands of trees transform decision trees into high performing, stable predictors [@Breiman1996].

The Random Forests algorithm works by training multiple trees from the bootstrapped samples of a dataset. Additionally, when creating each tree, a random subset of attributes (covariates) is considered at each split. The reduced subset of covariates is resampled for each split in the tree. Dominant covariates are then suppressed for a fraction of the splits, allowing the algorithm to explore signals in weaker covariates [@Breiman2001a]. Performance statistics and tests run within the algorithm provide insights to the modeller, such as variable importance and variable relationships [@Sealfon2012].

##### 3.1.3 Gradient Boosted Trees

The boosted decision tree approach applies gradient descent theory to a series of decision trees. Performance tends to be higher if individual trees are limited to a certain depth to maintain simplicity and prevent overfitting [@Elith2008]. Unlike Random Forests, the trees are sequential, where each tree models the residuals (or errors) of the preceding tree.

By modeling the errors, misclassified cases are weighted higher than correctly classified cases and influence the structure of the next tree more [@Hastie2009]. This increased weighting of error cases is called boosting. The successive results of up to thousands of trees is very powerful [@Elith2008]. @Caruana2006 tested Boosted Trees, Random Forests, Neural Networks, SVM's, Logistic Regression and Naive Bayes on 11 binary classification problems and found that Boosted Trees performed best, followed by Random Forests. This demonstrates how capable ensemble tree methods are of competing with high-level machine learning algorithms.

##### 3.1.4 Naive Bayes

Naive Bayes is a simple, fast method included in this work as it provides a good baseline method against which to compare complex methods [@Caruana2006]. It works by making conditional independence assumptions about the explanatory variables in order to simplify probability calculations for a categorical response variable. For example, if predicting a binary variable, $R$ (with classes 0 or 1) given two covariates $X_1$ and $X_2$, the equations for the probability of both outcomes is as follows:

$p(R = 1|X_1 X_2) = p(X_1|R = 1) \cdot p(X_2|R = 1) \cdot p(R = 1)$

$p(R = 0|X_1 X_2) = p(X_1|R = 0) \cdot p(X_2|R = 0) \cdot p(R = 0)$

These equations are simple to compute given the data, and the class with the highest probability can then be chosen [@Provost2013]. The method can perform well in real world tasks because the assumption of independence does not significantly damage predictions, but amplifies the magnitude of the probability [@Provost2013]. This is fine for ranking class 0 against 1 but the output probabilities are not accurate statistical probabilities [@Caruana2006].


##### 3.1.5 Model Blending

Several research groups involved in the high profile data science competition, Netflix Prize, developed sophisticated methods of model stacking, otherwise known as ensemble methods or model blending [@Sill2009]. In this paper, the term 'blending' will be used to avoid confusion with ensemble tree methods. Model blending combines predictions from several models that used methods with different theoretical foundations. In this way, the strengths of each method can be combined. Since the 1990's research in statistics has shown that averaging results from different methods can provide better predictive accuracy than any single model [@Madigan1994].

Six blending methods, ranging from simple to complex, were chosen. These included simple averaging of the individual model results, building a Logistic Regression model and Boosted Tree model using individual model results only, feature weighted linear stacking (FWLS), Random Forests, and Boosted Trees. The first two models are simply the average, and weighted average of predictions from separate individual models. For example, if the Logistic Regression model output a probability of 0.6 for one case, while a Boosted Tree output 0.7, a simple blended average for that case would be a probability of 0.65. The last three methods are more complex, and take advantage of potentially meaningful interactions between the individual model predictions and the original covariates describing each case (called meta-features in the model blending literature) [@Sill2009]. For example, if the Logistic Regression model predicted profitability better than the Boosted Tree model for projects with only one team member, the complex blending methods would take advantage of this interaction. In summary, in the complex methods, the probability outputs from each model are interacted with each of the original covariates. Model blending has the potential to elevate output from the original methods by combining each of their strengths.


#### 3.2 Procedure

The raw data from the case study company's database consisted of three separate datasets: one describing the client and technical details for each project, another with each employee's time sheet entries, and another with all past invoices. This data was combined, cleaned, and rearranged so that each project was described within one row. From the timesheet data, 10 variables were engineered including the percent of hours performed by 'professional' employees as opposed to 'technical' employees, and the position of the employee that completed the most hours on each project. 3 variables were engineered from the invoicing data such as total amount invoiced for a project and mean invoice size for a client. Finally, from the project dataset, 2 variables were engineered including the number of projects completed with a client, and project category. Text analysis of the project description determined the project category variable. The response variable, whether the project was profitable or not, was calculated by subtracting the cost of employee hours from the timesheets from the sum of the paid invoices for a project. The total amount invoiced for projects ranged from \$500 to over \$1,000,000.

34 covariates made up the full dataset, and these were narrowed down to 11 in order to reduce predictive noise and provide a simple model that is comprehendable for stakeholders [@Weisberg2005]. Variable selection methods included linear regression, random forests, and cforests and the final variables are listed below.

* % of Hours Completed by a Professional-level Employee
* Time span
* Number of Employees on Project
* Amount Invoiced for Project
* Business Category of Client
* Project Category
* Total Amount Invoiced from the Client - Past Jobs
* Discipline
* Position of Main Project Employee - new variable from ANOVA findings
* % of Hours by Main Project Employee
* Billing Type


Timespan and the total amount invoiced are unknowns at before a project commences. Therefore, these variables were discretised into 5 or 6 categories from which a manager would be able to select. The category boundaries were determined by visualising clusters using hierarchical dendrograms.

##### 3.2.1 Research Question 1

To answer the first research question, if loss making projects can be predicted using statistical and machine learning algorithms, the binary classification and blending methods were built and evaluated. The area under the receiver operating characteristic (ROC) curve (AUC) statistic was used to compare models and assess performance. Models were built on a training subset of data (80%) and validated on the remaining data. Therefore, different training and testing sets produced different AUC statistics. In order to compare which models performed significantly better than others, an adequate sample size of AUC statistics was required. A two-sample power calculation run on 20 AUC output for each model determined that 100 samples were required to achieve a statistical power of 0.8.

Then, the model output from the validation sets were combined and fed into the three simple and three complex blending models. Again, 100 models were built to achieve a power of 0.8 for the variation in AUC test statistics across different divisions of training/testing data. A maximum of five training and validation divisions were created from a complete set of individual model results.

##### 3.2.2 Research Question 2

The second research questions asks whether a predictive algorithm can improve a company's profitability? This was analysed using a simple decision-making scenario. The model with the highest AUC score may not improve company profits the most, because this depends on *which* loss making and profitable projects are accurately being identified. Therefore, all blended models as well as individual models were carried forward for this analysis. A profit curve was built to compare methods, which is a chart that plots the change in profit the company earns on the y-axis vs. the probability threshold on the x-axis. For this decision making scenario, the threshold is the probability output value (a value between 0 and 1 that the model predicts for each case), from which it is decided whether a project will be accepted or rejected. For example, if the output threshold is 0.6, and a model predicts a project has probability of being a loss-making job of 0.7, the project is not taken on by the case study company. All profits and losses from jobs above the threshold were discounted. The y-axis of the profit curve is the ratio of profit after rejecting projects based on model output to historical profit:


$$
\begin{aligned}
Ratio\ of\ Model\ Profit\ to\ Historical\ Profit\ (\%) &= \frac{\sum_{p = 1}^{N} I(Pr_p < threshold) \cdot Profit_p}{\sum_{p = 1}^{N} Profit_p} \cdot100
\end{aligned}
$$


<!-- \Delta\ Overall\ Profit\ \textrm{(%)} = \frac{\sum_{p = 1}^{N} I(Pr_p < threshold) \cdot Profit_p}{\sum_{p = 1}^{N} Profit_p} \cdot100 -->

Where

$$
\begin{aligned}
I(\cdot) &= \textrm{the indicator function} \\
N\ &= \textrm{the number of individual projects that are being included in the analysis} \\
Pr_{p} &= \textrm{probability output from the algorithm for project}\ p \textrm{. Values are}\\
& \qquad \textrm{betweeen 0 and 1 where 1 is loss making)} \\
threshold &= \textrm{a chosen value between 0 and 1.}\ \Delta\ Overall\ Profit\ \textrm{is calculated for} \\
& \qquad \textrm{several}\ threshold\ \textrm{values which defines the profit curve} \\
Profit_p &= \textrm{profit for individual project}\ p
\end{aligned}
$$

If the threshold were zero, all jobs were rejected, the profit would be $0, and the ratio to historical profits would be 0%. If the threshold was 1.0, all jobs were accepted and the profit would be the same as historical profits, giving a ratio of 100%. The aim was to find the optimal threshold point where mostly loss-making projects were rejected, resulting in higher profits. 100 profit curves were built for each method to achieve a power of 0.8 and a 95% confidence interval could also be determined around each threshold point. The final expected increase in profit combined with the percentage of projects to be rejected presents a clear scenario that the case study business managers could strategically assess for implementation.


4. Prediction Results and Discussion
====================================

Present results clearly and concisely
discussion should explore the signficance of the results of the work, not repeat them.

#### 4.1 Individual Models

The predictive formula was re-structured so that the 11 explanatory variables predicted a new response variable: profitable/unprofitable projects.
The five prediction methods, were initially built without imputing data, by using the maximum amount of data possible depending on the method. Boosted Trees and Naive Bayes are able to process data with missing values, so all data could be input [@gbm; @e1071].
100 models of each method were built. The violin plot below summarises the AUC values produced by each method and the 'violins' are coloured according to whether the distributions significantly vary to Boosted Trees using a critical value of 0.05.

```{r reduced_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the methods when predicting profit/loss. Subsets of the data were used for Logistic Regression and Random Forests in order to provide datasets without missing values.")}

reduc.totAUC <- readRDS("reduc_df_auc.rds")

#significant difference??
index = c(1,3,4)
pvals = data.frame("method" = c('log','rf','naiive'), 'p_value' = NA)
for(i in seq_along(index)){
        pvals$p_value[i] = t.test(reduc.totAUC[[2]], reduc.totAUC[[index[i]]])$p.value %>%
                format(digits = 2) %>% as.numeric
}

signif = pvals %>% filter(p_value < 0.05) %>% .$method
reduc.totAUC = melt(reduc.totAUC)
reduc.totAUC$sigdiff = ifelse(reduc.totAUC$L1 %in% signif, TRUE, FALSE)


ggplot(reduc.totAUC, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = sigdiff, colour = sigdiff), alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", shape =3) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        scale_fill_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from Boosted Trees") +
        scale_colour_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from Boosted Trees") +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 models of Each Method") +
        scale_x_discrete(labels = c("boost" = "Boosted Trees", 
                         "log" = "Logistic Regression",
                         "naiive" = "Naive Bayes", "rf" = "Random Forest")) +
        theme(legend.position = "bottom")
        

        
```

The AUC performance of Logistic Regression and the Random Forest algorithms cannot be statistically differentiated from Boosted Trees.

Next, data imputation methods were trialed which would make the complete data set available to Logistic Regression and Random Forests. Again, 100 models were required to achieve a power of 0.8 with respect to Boosted Trees

```{r mice_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the methods when predicting profit/loss. Each method was fed the same imputed full dataset.")}


mice.totAUC <- readRDS("mice_df_auc.rds")

#significant difference??
index = c(1,3,4)
pvals = data.frame("method" = c('log','rf','naiive'), 'p_value' = NA)
for(i in seq_along(index)){
        pvals$p_value[i] = t.test(mice.totAUC[[2]], mice.totAUC[[index[i]]])$p.value %>%
                format(digits = 2) %>% as.numeric
}

signif = pvals %>% filter(p_value < 0.05) %>% .$method
mice.totAUC = melt(mice.totAUC)
mice.totAUC$sigdiff = ifelse(mice.totAUC$L1 %in% signif, TRUE, FALSE)

ggplot(mice.totAUC, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = sigdiff, colour = sigdiff), 
                    alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", shape =3) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        scale_fill_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from boosted trees") +
        scale_colour_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from boosted trees") +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 Models of Each Method - Imputed Data") +
        scale_x_discrete(labels = c("boost" = "Boosted Trees", 
                         "log" = "Logistic Regression",
                         "naiive" = "Naive Bayes", "rf" = "Random Forest")) +
        theme(legend.position = "bottom")
```

Boosted Trees, Logistic Regression, and Random Forest all performed significantly better than the baseline algorithm, Naive Bayes, however none outperformed Boosted Trees.

Testing Boosted Trees, Random Forests, Naive Bayes, Logistic Regression and Bayesian Networks on the binary classification problem showed that Boosted Trees, Logistic Regression and Random Forests performed best (according to AUC). Naive Bayes and Logistic Regression were included as baseline models and it was expected that the more complex models would outperform these. Therefore it was surprising that results from 100 Logistic Regression models were not statistically significantly lower than Boosted Trees.

A possible explanation for this, according to the literature, is that there was not enough data for the ensemble trees to learn the complex decision rules at which they excel. Trees tend to overfit the patterns in a smaller training set. Logistic Regression on the other hand is capable of only one decision boundary (which does not have to be parallel to the variable axes) and is not prone to overfitting [@Perlich2003]. This may explain Logistic Regression's comparatively high performance on the case study's small but complex data set.

Boosted trees, Random Forests and Logistic Regression all had mean AUC values between 0.755 to 0.764. In summary, the individual binary classification models performed well above random chance (AUC = 0.5). Whether the model is worth implementing in the work place is dependent on the extent to which the algorithm would improve 'bottom line' profits for the business and if the model can affect decisions in practice.

- NB and BN were bad - one sentence

#### 4.2 Blended Models

Blended models combine the predictions from each high-performing individual model to create averaged or 'blended' predictions. In this case, the Random Forest, Boosted Trees, and Logistic Regression models were chosen. Through variable importance studies, 6 variables were included as metafeatures. All six methods were compared against the original Logistic Regression model's AUC distribution (as it was not statistically different from the ensemble trees)


```{r blend_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the blending methods when predicting profit/loss. 100 models were built for each method.")}

auc.list <- readRDS("blend_auc2.rds")
means = lapply(auc.list, mean) %>% unlist
greater = means[means> means["orig.log"]] %>% names
auc.melt = melt(auc.list)
auc.melt$greater = ifelse(auc.melt$L1 %in% greater, TRUE, FALSE)


ggplot(auc.melt, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = greater, colour = greater), 
                    alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", 
                     shape =4) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        theme(axis.text.x=element_text(angle=40,hjust=1)) +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 Blended Models of each Method") +
        scale_x_discrete(labels = c("boost" = "BT Blend", 
                         "log" = "FWLS", 
                         "average" = "Simple Average Blend",
                         "orig.log" = "Orignal LR", 
                         "rf" = "RF Blend",
                         "simp.boost" = "Simple BT Blend",
                         "simp.log" = "Simple LR Blend")) +
        scale_fill_manual(values = c('navajowhite','sienna1'),
                          name = "AUC Greater than Original Logistic Regression") +
        scale_colour_manual(values = c('navajowhite','sienna1'),
                          name = "AUC Greater than Original Logistic Regression") +
        theme(legend.position = "bottom")


```

The above plot shows that 4 methods had a higher mean AUC than the original Logistic Regression model: 

* the simple blended Logistic Regression 
* the simple average 
* and the two Boosted Tree models.

The simple averaged method and simple logistic regression could achieve a statistical power of 0.8 with 150 samples (models), whereas the boosted trees required thousands. The additional 50 models were run for the simple logistic regression and simple average to reveal similar distributions that were indeed statistically significantly higher than the individual logistic regression AUC distribution. P-values from two sample t-test were 0.0019 and 0.0021 respectively.

 Blended models improved the mean AUC from 0.759 to 0.77, which is an increase of `r ((0.77-0.759)/0.759) %>% "*"(100) %>% round(1)`%. It was expected that model blending would improve predictions because it combined the strengths of individual models with different theoretical foundations.
 
 The trials of different blending methods demonstrated that again, the simplest methods worked best with the case study data. In this incident, averaging the results of the best three single models (Logistic Regression, Random Forests, and Boosted Trees) or taking a Logistic Regression of the three models outperformed more complex Boosted Trees, Random Forests, and Logistic Regression blends that facilitated interaction of the individual model results and original variables (meta-features). Simple blended models could perform better than complex methods if the data is not big enough for the complex models to learn the more intricate patterns at which they excel. This was observed in the single model methods. their success in the 2009 Netflix competition generated some publications. The Netflix data set comprised of almost 3,000,000 observations, so the size of the data could have enabled the success of complex blending methods such as FWLS [@Sill2009]


5. Business Impact
==================

This chapter first presents the full range of method results in terms of improvements to the case study's bottom line. A business decision making scenario was created so that profit curves based on the decision rule could be built and analysed. From the profit curves, optimal probability thresholds could be derived for each method (individual and blended methods). The predictive models output a 'probability' between 0 and 1 that each project will be a loss making job (where probability = 1 indicates a loss making job). The question the arises, at what probability would a decision-maker round the probability up to 1 or down to 0? And what business decision would then be made?  To find the threshold point for rounding, an experimental business-scenario was tested. At a given threshold, all projects with probability outputs above the threshold were considered too risky, and were rejected. All profits and losses from these projects were forfeited, while the profits from the remaining jobs (below the threshold) were summed to give a revised total profit. This profit calculation was made on a range of thresholds between 0 and 1 at 0.05 increments. The total profits were plotted for each threshold and joined to make a profit curve

The plot below illustrates the distribution of profit curves for each method, where the solid lines join the mean values at each threshold point. The grey ribbon illustrates the 95% confidence interval for the profit improvement ratio at each threshold point for the 100 models.

```{r profit_curve, fig.height = 6, fig.cap = c("Profit curves summarising results from 100 models of 9 methods: 3 simple blends, 3 complex blends, and the original 3 best methods.")}

finance.plot = readRDS("plot_finance_100_4.rds" )
finance.plot$method <- factor(finance.plot$method, 
                                    levels=c("average", "simp.log","simp.boost",
                                             "comp.boost","comp.rf", "FWLS","orig.log",
                                             "orig.rf","orig.boost"))

ggplot(finance.plot, aes(x= threshold, y= profit.ratio, colour = method)) + 
        # geom_point(alpha = 0.005, shape = 16, size = 1) + 
        geom_hline(yintercept = 100, colour = "darkgray", linetype = 2) + 
        geom_hline(yintercept = 0, colour = "darkgray", linetype = 1) +
        stat_summary(fun.y = mean, 
                     fun.ymin = function(x) mean(x) - 1.96*sd(x),
                     fun.ymax = function(x) mean(x) + 1.96*sd(x),
                     fill = "gray", geom= "ribbon", alpha = 0.7, colour = NA) +
        stat_summary(fun.y = mean, geom="line") +
        scale_y_continuous(breaks = seq(0,110, by = 20)) + 
        scale_x_continuous(breaks = seq(0,1, by = .2)) +  
        theme(legend.position = "none",
              panel.background = element_rect(fill = 'gray95')) +
        labs(x = "Probability Threshold", 
             y = "Ratio of Model Profit to Historical Profit (%)",
             title = "Profit Curve for Blended and Individual Models") + 
        facet_wrap(~method, 
                   labeller = labeller( method = c("average" = "Simple Average Blend",
                                       "orig.boost"= "Orig. Boosted Trees",
                                       "orig.log" = "Orig. Logistic Regression",
                                       "orig.rf" = "Orig. Random Forest",
                                       "simp.log" = "Simple Log. Reg. Blend",
                                       "simp.boost" = "Simple Boosted Tree Blend",
                                       "comp.boost"= "Complex Boosted Tree Blend",
                                       "comp.rf"= "Complex Random Forest Blend",
                                       "FWLS" = "FWLS"
                                       )))

```

Two blended models clearly outperformed the individual models as shown in the above plot with higher profit ratios as well as tighter confidence intervals. The simple Logistic Regression blend performed best with the highest mean profit ratio of 109% with a standard deviation of 1.28%. %. This means that for the simple blended Logistic Regression model, if all jobs above the probability threshold 0.6 were rejected, the profits would increase on average by 9% in comparison to historical profits.

Profit curves from the simple average blend and Logistic Regression blended models outperformed the complex blended models, which follows logically from their significantly higher AUC distributions. The simple average blended model produced a profit curve with an almost identical profit improvement (and standard deviation) to the Logistic Regression blend. In the data, projects assigned a probability higher than 0.6 represent only 4.3% of all projects.

The shaded grey 95% confidence intervals in the Profit Curve plot shows that with the 100 sample models that were analysed, none of the lower bounds for the original methods' or complex blending methods were above 100% on the y-axis. That is, it cannot be said with 95% confidence that the original methods would produce a mean profit higher than historical profits in the given decision framework. Clearly this level of certainty is not beneficial for the case-study business to adopt.

It is not clear why simple linear models and the averaging model outperformed ensemble tree blending methods. As previously stated, the ensemble trees may not have received enough data to adequately learn the complex series of rules they develop. The only differences between FWLS and the simple Logistic Regression (that performed best) were two additional explanatory variables and four interaction terms. The number of variables was not high for the amount of data since according to @Peduzzi1996, 10 events per explanatory variable or more avoids the risk of biased estimation of variable coefficients in Logistic Regression. The data contained 315 events per explanatory variable and did not pose that risk. Nevertheless, the additional variables must have added misleading noise to the model.

To conclude, because of the promising AUC results, it was logical that the models translated into financial benefits for the company. The 9% improvement in profits produced by the simple blended Logistic Regression is reasonable, and may be high enough to trigger further cost-benefit analyses and the development of a more comprehensive framework describing different decision scenarios.

- decision scenario

- profit curves

- prototype decision support interface


6. Conclusions and Future Work
==============================

3/4 page

The general aim was to use statistical techniques to predict the profitability of projects for a case study consulting business using their internal CRM data. This was rigorously completed by approaching the prediction of 'profitability' as a binary classification problem predicting either profit or loss. Several statistical and machine learning approaches were applied including Naive Bayes, Bayesian Networks, Linear Regression, Random Forests, gradient Boosted Trees as well as multiple methods of blending the individual models' output.

AUC = 0.76). This was achieved by 3 individual methods while various techniques that blended the individual methods improved results further (AUC = 0.77).

predictive models could be shown to improve the overall profitability (bottom line) of the case study business. A range of probability threshold values were trialed for each method using a decision framework where projects scored below the threshold were accepted, while projects above the threshold were rejected. If a project was rejected, the profits and losses were forfeited, and the remaining accepted profits and losses were summed.

Final results showed the simple Logistic Regression blend of the individual Logistic Regression, Random Forest and Boosted Tree models improved profits the most. The 95% confidence interval for these improvements was between 6.5% and 11.5% using a probability threshold of 0.6 (approximately 4.3% of projects).

These results contribute significantly to the research in cost estimation in three ways: the applied methods, the decision framework, and the appeal to user trust. Ensemble tree methods and blending had been applied minimally to cost estimation previously, even though ensemble trees provide insight into model structure while predicting at a similar level to Neural Networks. Next, previous studies have verified predictive accuracy but stopped short of how the algorithm would affect decisions and what the measured benefits would be. This study presented a clear framework for how a business could improve profits by applying the algorithm.

Further work is required to test user confidence in the output. Another topic identified for future research was to test how well managers estimate some numeric project input variables. In particular, time span, team size, total invoiced amount, and percentage of hours completed by professionals should be tested as these variables were calculated post project completion. Time span and total invoiced amount were discretised into wide categories, which should be easier for a manager to choose between.

future work: user interface.

Overall, this work has successfully built a mathematical blend of Logistic Regression, Random Forests, and Boosted Tree models, from a consulting company's internal project data. This blended model can predict whether a project will be profitable or not and in a reasonable decision framework, can guide managers in rejecting financially risky projects and improving profitability of the business.

Acknowledgments
===============

This research was supported by a scholarship/ ACEMS?

References {#references .unnumbered}
==========

#### Installation

If the document class *elsarticle* is not available on your computer,
you can download and install the system package *texlive-publishers*
(Linux) or install the LaTeX package *elsarticle* using the package
manager of your TeX installation, which is typically TeX Live or MikTeX.

The author names and affiliations could be formatted in two ways:

(1) Group the authors per affiliation.

(2) Use footnotes to indicate the affiliations.

Bullet points.

-   document style

-   baselineskip

-   front matter

-   keywords and MSC codes

