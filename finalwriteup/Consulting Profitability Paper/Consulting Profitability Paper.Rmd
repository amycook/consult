---
title: Predictive Models to Support Quoting of Fixed Fee Consulting Projects
author:
  - name: Amy Cook, Paul Wu, Kerrie Mengersen
    email: a21.cook@qut.edu.au
    affiliation: Queensland University of Technology
    footnote: Corresponding Author
address:
  - code: Queensland University of Technology
    address: School of Mathematical Sciences, George Street, Brisbane, QLD, 4000
abstract: |
  A concise abstract is required. limit to 250 words. clearly state purpose of research, principal resutls and major conclusions. no refs
  
  Engaging in loss making jobs for fixed fees is a major problem in consulting, particularly in the competitive construction industry. This thesis investigates whether machine learning techniques applied to a company's passively collected internal data could help avoid loss making jobs or help tactfully choose when to enforce stricter contracts. It was found that in a specific decision framework, a case study's profits could be improved 9% by declining approximately 4% of projects. Alternative decision frameworks are also proposed and evaluated. Algorithmic methods such as Logistic Regression, Random Forests, Boosted Trees, Naive Bayes, and Bayesian Networks were applied as well as blended combinations of these methods. A decision scenario which rejected projects above a sequence of tested thresholds was run in order to find the optimal threshold for profit improvements. The blended Logistic Regression model outperformed other methods and produced a 95% confidence interval of 6.5 - 11.5% profit improvements. The findings from this research have the potential to assist managers in reducing losses by highlighting risky projects and guiding project-based changes to fee structures.
keywords: |
  consulting; machine learning; profitability; predictive model; construction industry; data mining
abbreviations: 
  - name: Customer Relationship Management
    footnote: Abbreviations

bibliography: Library.bib
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_

```{r, echo=FALSE, include=FALSE}

library('knitr')
library('ggplot2')
library("plyr")
library("dplyr")
library('magrittr')
library('reshape2')
library("rpart")
library('car')
library('e1071')
library('randomForest')
library('RColorBrewer')
library('pwr')
library('scales')
library('purrr')


# detect OS
if(.Platform$OS.type == 'windows'){
        # setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
        opts_knit$set(root.dir= "C:/Users/n9232371/Documents/github/consult/finalwriteup/report_data")
} else{
        # setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
        # setwd("~/Documents/github/consult/finalwriteup/report_data")
        opts_knit$set(root.dir= '~/Documents/github/consult/finalwriteup/report_data')
}

opts_chunk$set(fig.width=6.7, dpi = 200, warning = FALSE, message = FALSE, echo = FALSE, warning = FALSE)
```

1. Introduction
==========================

Clearly state the research question and objectives of the work. Briefly provide any necessary
background to frame the research question. Concisely summarize the major findings/results.

Many consulting projects can be financialy risky endeavors for the consultant, often depending on the industry. Engineering consultanting companies in the construction industry complete a wide range of complex projects in a competitive enviroment. Because of the way contracts are structured, many of these projects result in losses. The purpose of this study was to test whether these loss making projects could be predicted by statistical and machine learning algorithms using the information available at project conception. Secondly, could the predictive algorithm improve a company's profitability? Predictive models and value frameworks were built using project data from a case study engineering company to answer these questions.    

1 page


#### 1.1 Problem motivation

Financial risk is taken on by a consulting company when they offer a fixed fee to complete a project. In competitive climates, clients are in the position to request fixed price quotes from several consultants before a project is awarded. Hence, initial quoted fees must be competitive but also accurately cover consultant salaries and business costs. 

How do consultants make this accurate yet competitive fee estimate? This varies from industry to industry, but typically a consulting manager has experience in the type of project they are quoting and, after reviewing the project details, can use a combination of intuition, analysis of project details, and rules of thumb. Unfortunately, some projects priced using this method can blow out of budget, causing significant financial losses to the consultant.

Cost blowouts of complex projects has a history, particularly documented in the It and construction industry. A review of 6 IT project surveys between 1984 and 1994 by Moløkken & Jørgensen (2003) revealed that 60-80% of IT projects encountered effort and/or schedule overruns, where the average overrun was by 30 to 40%.
A study on large-scale infrastructure projects over the past seventy years revealed that cost forecasts consistently underestimated the cost of rail projects by an average of 44.7%, the cost of bridge and tunnel projects by 33.8% and the cost of road projects by 20.4% [@Flyvbjerg2007].

Kahneman won a nobel prize in 2002 for his research into decision making on this topic. He argued that a person's natural optimistic view of their own skills leads to consistent underestimation of the time and risks involved in a project [@Lovallo2003]. To counter this phenomenon, research by Flyvbjerg [-Flyvbjerg2011] advised that managers should be confronted by the financial performance of similar projects to the one being costed so that they take on a more realistic view of their abilities. This was based off a body of findings where people made better predictions about themselves after being exposed to facts about other people. For example, subjects were asked to predict their skill levels before and after being exposed to a summary of other people's skills, and their predictions after were significantly more accurate [@Lovallo2003].

This work aims to help managers provide better fixed price quotes by predicting whether a new project will be profitable or not, based on an algorithm trained on all previous project data from the same company. The manager could take multiple actions based on the algorithms output including not tendering for a project predicted to be loss-making, providing a high fixed price, or restructuring the contract so that it is not 100% fixed price. This research tests the simple case of rejecting all projects predicted as loss-making.


#### 1.2 Case Study

Project data from a single case study consulting company in the construction industry was used to build the predictive algorithm, trial several methods, and test their impact on the company's bottom line. Twelve years of passively collected data that described each project in terms of their clients, invoice history, employee hours and technical details was scraped from the company's internal database. Customer relationship management (CRM) software provided accesses to the database to all employees. Employees completed daily timesheets, alotting their hours to certian projects. Project managers input technical information and client details for each project, while the finance team tracked invoicing and additional project costs such as printing and transport in the same system. On top of this information, the CRM had the hourly cost of each level of employee. The cleaned dataset consisted of 2364 projects between 2004 and 2012. 70% of projects were fixed price and 20% of all projects made a loss after taking into account business costs.

Once several methods were built, to assess whether the model would improve the case study company's profitability, a simple managerial decision framework was applied: if the algorithm predicted the project to be loss making, the project was considered 'rejected' and all profits or *losses* of rejected projects were discarded. The remaining projects' profits and losses were summed to give a revised overall profit over the last 12 years of projects. This was compared to the summed profits and losses from all projects completed over the last 12 years. The best algorithm rejected enough loss-making projects to improve overall profits by 9%.

<!-- **General Aim** Use statistical techniques to model the profitability of projects for consulting businesses using their internal CRM data. Research will focus on a case study Engineering consulting company that offers their expert advice (in the currency of time) to business clients.  -->
<!-- The project outcomes are intended to assist the business in predicting project profitability before engagement. Several statistical and machine learning techniques were tested, compared and refined. -->

<!-- **Hypothesis 1** -->
<!-- A statistical or machine learning model based on historical project data can predict the profitability of a new project with greater accuracy than a baseline predictor. A baseline predictor for this project is one that predicts the average of a numeric response variable for all cases or, if the response variable is categorical, randomly assigned categories for each case, where the proportion of assigned categories matches the true categorical proportions. -->

<!-- **Hypothesis 2** -->
<!-- The predictive model built from Hypothesis 1 can be shown to have a positive impact on the overall profit earned by the case study business. The overall profit is represented by the following equation: -->


2. Literature Review
====================

<!-- Summary of Key Related Research  -->
<!-- This section should include a brief summary of key related research. Emphasis should be on demonstrating the foundation for the current investigation. Specifically, the goal is to  -->
<!-- clearly delineate a gap or missing link that the current  -->
<!-- research fills. Authors should avoid presenting a litany of past research and should focus -->
<!-- on prior work necessary to demonstrate the -->
<!-- existence of the research gap addressed in the manuscript. -->

For the case study company, training the algorithm to predict actual fee was not possible because the projects were too varied, and there was inadequate information describing the size and scope of each project in the database. However, in the literature, similar studies typically did attempmt to predict project costs of IT consutling projects and final construction costs of buildings and infrastructure.

#### 2.1 Cost estimation in the Construction Industry and IT Industry

Previous studies in the literature have focussed on reporting the predictive accuracy of their cost estimation models for IT or construction projects using data from multiple businesses or governments around the world. In contrast, this research is focussed on determining how or if models predicting project profitability can improve the bottom line for a case study business, given data the company has internally generated. 

Reviewing a range of individual studies show that cost estimation models have generally been built from at most 530 projects [@Kim2004] [@Finnie1997] - 299 software development[@Pai2013] - 163 software [@Shin2015] - 234 building projects, and often under 100 projects [@Finnie1997] [@Chan2005] 87 buildings. Sometimes it is not stated where the project data came from, and some studies specified the information came from surveys, with responses from companies and governments around the world. In this study, the dataset originates from a single company's database, which means the data is easily accessible to the company and there are 2364 projects, which is much more than previous studies. A study by @Mendes2004 demonstrated that using within company data was also more accurate than across company data. Their dataset consisted of 67 across company web projects and 14 within company projects. 

Almost all past studies analysed their model performance by predictive accuracy (reported as R^2 or RMSE) of cost predictions [@llist some stff] and compared these statistics bewteen methods. However, these results were not translated into a business case for the impact these models could have on an institution or how the model could be practically integrated into decision making. For example, if an institution used the model, how would it be applied to their decision making process and what magnitude of benefits could they expect? Saradhi & Palshikar's [-@Saradhi2011] work on employee churn, where 'churn' refers to the number of individuals moving out of a group within a certain time, presented a good framework for implementing their model. Theirsupport vector machine (SVM) predictive model identified employees at high risk of churn. Then a method for determining the value of each employee in terms of their importance to projects and their monthly chargeability was described, which enabled high risk employees to be ranked. Managers then had a clear direction for who to target first to prevent churn. This extension provided a comprehensive framework for how business managers could adopt a machine learning algorithm to improve business operations. It was a valuable addition that is absent from most cost estimation research, but will be included in this work.

#### 2.2 Methods used in Cost Estimation and Other Business Applications

Cost estimation literature tended to apply linear regression, neural networks and sometimes SVM methods, while predictive methods applied to other business problems investigated a wider range. This study aims to compare a wider range of methods to past cost estimation studies.

In the construction industry, often only one method was assessed, such as linear regression, without comparison to other methods [@Chan2005; @Elfaki2014]. Neural Networks started appearing in literature in the 1990's and @Elfaki2014's review of cost estimation methods from 2004 to 2014 found that artificial Neural Networks and SVM's were the most popular machine learning techniques, but studies tended to only test one method. Some studies showed that Neural Networks outperformed Linear Regression, however other studies established they are approximately equal [@Kim2004; @Attalla2003]. To the authors' knowledge @Shin2015 documented the only application of ensemble tree methods (Boosted Trees) to cost estimation in construction projects. They found Boosted Trees slightly outperformed Neural Networks, but not significanatly.

In the software cost estimation literature, Linear Regression was the most popular method, however multiple studies showed that Neural Networks definitively outperform regression models  [@Finnie1997; @Pai2013; @Matson1993]. Despite the higher performance, Neural Networks were criticised for not providing reasoning or structure behind their predictions (black-box predictor) [@Finnie1997].

The review of methods applied to the construction and IT industry highlighted the use of Linear Regression, Neural Networks, SVM's and in one case Boosted Trees, however research on other business problems utilised a wider range of methods. In financial credit scoring, a study by @Brown2012 found that Random Forests and Boosted Trees consistently outperformed Neural Networks in classification. In addition, ensemble trees such as Random Forests and Boosted Trees can output partial dependency plots and variable importance measures that provide insight into the model's calculations. This is a valuable asset oer black box predictors. @Kumar2007 performed a detailed review of statistical and machine learning techniques applied over 37 years in the context of bankruptcy prediction in banks and reported that although SVM's performed well, they are often complex and slow, requiring a great deal of memory. They also assessed method blending techniques, which refers to combinations of two completely different algorithms, and found they can often outperform individual methods such as Linear Regression or Boosted Trees alone.

To date, Linear Regression, Neural Networks, and Support Vector Machines (SVM's) have been comprehensively applied to cost estimation. This research does not test black-box predictors (Neural Networks and SVM's) because insight into the model's predictions is crucial for facilitating user uptake in the case study company. Ensemble tree methods such as Random Forests and Boosted Trees will be used instead, as they can perform as well as Neural Networks while providing insight to their output [@Caruana2006]. Blending of multiple predictive models is also applied in this work and has not yet been done in cost estimation. 

#### 2.3 Gaps

In comparison to previous studies, this project advances the body of work on cost estimation in two ways. First, it focusses on building a model that is practical and profitable for a business to implement by using the case study's interenal database as well as testing a managerial decision framework based off the model output. This framework presents a clear measure of expected benefits provided by adopting the profitability model into a business' decision process. Secondly, advanced predictive methods are trialled which have been minimally tested on this problem. These include ensemble decision tree methods, such as Random Forests and Boosted Trees, and blends of multiple models built from different methods.

 

3. Prediction Methods
=====================

Should provide sufficient detail to allow the work to be reproduced. Methods already published should be indicated by a reference: only relevant method modifications should be described.

Neural Netowrks and SVM were not purusued because black box
A previously mentioned disadvantage is that the NN algorithm is a black box because the internal structure is too complex for interpretation. They also require a lot of training data relative to other methods.

Also, SVM's can be very slow to train and therefore not suitable for industry purposes [@Auria2008]. Because it is so important to engage decision makers with a model that can explain its results, these two methods are not appropriate for the effort estimation problem. 

#### 3.1 Predictive methods

binary classification problems, where the objective is to predict the probability of an event occurring. Probability is a continuous response variable, however predictions from a linear regression would not be bound by 0 and 1.

##### 3.1.1 Logistic Regression

Because of the linear relationship, an equation can be fit to the log odds of the binary response variable against each of the explanatory variables using linear regression. Then, the linear equation describing the log odds can be transformed back to probability by taking the inverse log i.e. the exponential and rearranging [@Macdonald1975]:

The result in a sigmoidal function bound by 0 and 1. The sigmoidal function originates from a linear fit of the log odds in the data. Assume linear relationship between covariates and response variable

 A coefficient represents the change in the log odds of the response variable for each unit increase in an explanatory variable. Therefore, taking the exponential of the coefficient is the change in odds of the response variable. If the change in odds was 2, then if the explanatory variable increased by 1, the response variable event would be twice as likely to occur.
 
 Logistic Regression is a good  benchmark to compare other binary predictive models due to its simplicity and speed [@Moore1989]. single linear relationship means Not prone to overfitting

##### 3.1.2 Random Forests

single decision trees also suffer from low predictive accuracy and instability, so to combat these problems, ensemble tree methods were pioneered in the 1990's [@Breiman1996].

The three methods of combining hundreds or thousands of trees turn decision trees into high performing, stable predictors.

training multiple trees from the same data by sampling bootstrapped training sets with replacement. However, when creating each tree, a random subset of attributes (variables) is considered at each split. The reduced subset of attributes is resampled for each split in the tree. This allows dominant variables to be suppressed for a fraction of the splits, allowing the algorithm to explore signals in weaker variables [@Breiman2001a].

Additionally, ensemble tree methods are faster than SVM's and Neural Networks but can perform just as well and even provide insights such as variable importance and variable relationships [@Sealfon2012].

##### 3.1.3 Gradient Boosted Trees

Lastly, the boosted decision tree approach applies gradient descent theory to a series of decision trees. The trees are limited to a certain depth to maintain simplicity, and each tree models the residuals (or errors) of the preceding tree.
By modeling the errors, misclassified cases are weighted higher than correctly classified cases and influence the structure of the latest tree more [@Hastie2009]. This increased weighting on errors is why the algorithm is called boosting.
The limited depth of each tree prevents overfitting at each stage and the combined result of up to thousands of trees is very powerful [@Elith2008].

@Caruana2006 tested Boosted Trees, Random Forests, Neural Networks, SVM's, Logistic Regression and Naive Bayes on 11 binary classification problems and found that Boosted Trees performed best, followed by Random Forests. This demonstrates how ensemble tree methods are capable of competing with high-level machine learning algorithms.  

Trees are not built on a probabilistic framework, and therefore their results cannot be provided in this framework [@Louppe2014].

##### 3.1.4 Naive Bayes

The Naive Bayes method works by making conditional independence assumptions about the explanatory variables in order to simplify probability calculations for the response variable (the response variable must be categorical). 

These equations are simple to compute given the data, and the class with the highest probability can then be chosen [@Provost2013]. 

The advantages of this method are that the conditional independence assumption enables very fast calculations and predictions. The method can perform well in real world tasks because the assumption of independence does not significantly damage predictions [@Provost2013]. Assumptions of independence genearlly not true (correlated variables). This is fine for ranking but the output probabilities are not accurate statistical probabilities [@Caruana2006].

Naive Bayes method generally provides a good benchmark to compare against more complex models that should outperform it [@Caruana2006].

##### 3.1.5 Bayesian Networks

A Bayesian Network is a graphical probabilistic model that illustrates the conditional dependencies between variables in a data set. The model is visually represented by a directed acyclic graph (DAG) and is capable of linking the conditional dependency between any variable to another variable [@Heckerman1998]. The conditional relationships are Bayesian, where the probabilities in one node are conditional upon values in nodes directed towards it as well as preceding nodes

Bayesian Networks reduce computations required to find the probability of a unique combination given other explanatory variable values while still taking into account many conditional dependencies that Naive Bayes [@Barber2012]

Naive Bayes is the simplest form of a Bayesian Network [@Zhang2004]

Network relationships can be learned from the data, however this is not widely included as part of the suite of machine learning methods. The conditional variable dependencies are calculated from the data, which in turn can define the graph structure. Some conditional dependencies can be set before the structure is learned [@Barber2012].

Drawbacks of Bayesian Networks include the difficulty for them to process continuous variables.

Bayesian networks have found success in combining deterministic models with observed data as well as expert knowledge.[@Kragt2009]


#### 3.2 Procedure

 It includes first how the data was obtained, the lengthy cleaning process, 
 Projects varied from total invoiced amounts of \$500 to over \$1,000,000
 a project could have thousands of rows of relevant data that needed to be converted into a single row per project.
 10 engineered variables describing timesheets: Percent of hours performed by 'professional' employees as opposed to 'technical' employees, Position of the employee that completed the most hours on each project
 3 engineered from invoicing data: 
 4 from project data: number of projects completed with each client
 text analysis of project titles to develop project classifications
 
 Discretisation was performed by generating a hierarchical dendrogram of each variable to visualise the clusters.
 
Limiting the predictive model to a concise set of meaningful variables reduces noise and improves predictions. Less variables and a simpler model is easier for stakeholders to understand [@Weisberg2005]. followed by variable importance analysis, variable selection: varialbe importance aalyses done with linear regression, random forest, cforest to represent the range of algroithms that were trialled. 11 variables chosen:
 
 
 trials of selected predictive algorithms.  binary classification problem predicting profit or loss.  Complex models should be measured against simple models that can be built at a fraction of the computational cost.
 
 * Regression - baseline model
* Naive Bayes - baseline model
* Bayesian Network - grabage.
* Random Forest
* Gradient Boosted Trees

To compare the models, the area under the receiver operating characteristic (ROC) curve (AUC) statistic was used. An ROC graph visualises the curve from which an AUC score is calculated. In this problem a 'positive' is a loss making job. For binary classification, AUC is a more meaningful statistic than classification accuracy when the output is a probability that can be applied to the problem. The AUC indicates model performance across many thresholds while classification accuracy represents a single threshold.

a model must be first created using a training set. The model can then make predictions on the test set. this is used to build the ROC
A model that is perfectly classified would have an AUC = 1.
A model that predicts as well as random chance would have an AUC = 0.5. An AUC between 0.5 and 1 means the model is performing better than random chance.

In order to compare which models performed significantly better than others, an adequate sample size of results statistics is required. Multiple models could be made by using different data in the training vs. testing sets, each providing a resulting test statistic (RMSE or AUC).  Initially, 20 models of each method were created in this fashion. Then a two-sample power calculation was run using the two sets of 20 results to determine the sample size to achieve a statistical power of 0.8 (100 to achieve power of 0.8 between boosted trees and naive bayes)

Missing Data Imputation. All methods except gradient Boosted Trees and Naive Bayes could not handle missing data. a complete data set allows for complete sets of predictions from each method, and these predictions could then be blended to further improve results. The MICE Random Forest method was chosen for imputation because it has been proven to work well with complex data sets [@Shah2014]. If similar predictive results were obtained using Boosted Trees imputed data and imputed data, the imputed data must be reasonable. The imputed data set was then trialed on the remaining methods and compared to unimputed trials.
 
 Once the best methods were selected, they were blended in numerous ways, using both simple averaging techniques and sophisticated machine-learning algorithms. These were compared against individual models and the best constructs were selected. Six blending methods, ranging from simple to complex, were tested using predictions from the top performing individual models. These included simple averaging of the individual model results, building a Logistic Regression model using the individual model results only, a Boosted Tree model using individual model results only, feature weighted linear stacking (FWLS), Random Forests, and Boosted Trees. A simplified explanation of FWLS is as a Linear Regression where meta-features as well as model predictions from individual models are included as explanatory variables. Then, each meta-feature is interacted with each set of model results [@Sill2009].
 Feature interaction is performed passively due to the nature of how trees are built. A split in a node determined by one variable is conditional upon the preceding split, which was based on another variable and so on.
 The simplest method averaged the probability output (a number between 0 and 1) of all three models. The next two simplest methods consisted of building a Logistic Regression and Boosted Trees model from the three prediction model outputs only. Feature Weighted Linear Stacking (FWLS), Random Forest and Boosted Trees were also tested as blending methods in a more complex scenario where the predictions from each model became additional variables to the original explanatory variables (called meta-features in this context). The model predictions were interacted with each original variable so that models that performed more strongly under certain meta-feature states could be weighted as such.
 
 
Again, 100 models were built to achieve a power of 0.8 for the variation in AUC test statistics across different divisions of training/testing data. A maximum of five blended models were created from a complete set of test results (built from the individual models), then a new set of test results were created for the next five blended models.
 
 Finally the impact of the algorithm on the overall profits of the case study company was analysed via decision-making scenarios. This analysis differed from accuracy in predicting profitability, so several blended models as well as individual models were carried forward for this analysis.  This was done using a profit curve - a chart that plots the change in profit the company earns on the y-axis vs. the probability threshold on the x-axis. A simple approach was taken for this analysis, where projects with a probability to be a loss making job greater than the threshold were rejected entirely. Therefore all profits and losses from jobs above the threshold were discounted. An equation defining the change in profit as a percentage of the original profit using threshold rejection is shown below:



\begin{equation}
\Delta\ Overall\ Profit\ \textrm{(\%)} = \frac{\sum_{p = 1}^{N} I(Pr_p < threshold) \cdot Profit_p}{\sum_{p = 1}^{N} Profit_p} \cdot100
\end{equation}

<!-- \Delta\ Overall\ Profit\ \textrm{(%)} = \frac{\sum_{p = 1}^{N} I(Pr_p < threshold) \cdot Profit_p}{\sum_{p = 1}^{N} Profit_p} \cdot100 -->

Where

$$
\begin{aligned}
I(\cdot) &= \textrm{the indicator function} \\
N\ &= \textrm{the number of individual projects that are being included in the analysis} \\
Pr_{p} &= \textrm{probability output from the algorithm for project}\ p \textrm{. Values are}\\
& \qquad \textrm{betweeen 0 and 1 where 1 is loss making)} \\
threshold &= \textrm{a chosen value between 0 and 1.}\ \Delta\ Overall\ Profit\ \textrm{is calculated for} \\
& \qquad \textrm{several}\ threshold\ \textrm{values which defines the profit curve} \\
Profit_p &= \textrm{profit for individual project}\ p
\end{aligned}
$$

 
If the threshold was zero, all jobs were rejected and the profit would be $0. If the threshold was 1.0, all jobs were accepted and the profit would be the same as the profit the company actually experienced since the data is a sample of historic projects. The aim was to find the optimal threshold point where saying 'no' to a job above that level would result in higher profits, because jobs that were likely to make a loss were being rejected. This chart will clarify what percentage of profit increase the company could expect by integrating the algorithm into decision-making.

the curve varies with different divisions of the data. Therefore, in order to understand the uncertainty around the profit curve and to determine which blended or individual model performed statistically better than others, a large sample size of curves was required. 100 to achieve a power of 0.8. which was achieved by repeating 5-fold training/testing splits. A 95% confidence interval could also be determined around the highest point on each curve. The final expected increase in profit and the percentage of projects to be rejected presented a clear scenario that the case study business managers could assess in terms of their business strategy.
 


4. Prediction Results and Discussion
====================================

Present results clearly and concisely
discussion should explore the signficance of the results of the work, not repeat them.

#### 4.1 Individual Models

The predictive formula was re-structured so that the 11 explanatory variables predicted a new response variable: profitable/unprofitable projects.
The five prediction methods, were initially built without imputing data, by using the maximum amount of data possible depending on the method. Boosted Trees and Naive Bayes are able to process data with missing values, so all data could be input [@gbm; @e1071].
100 models of each method were built. The violin plot below summarises the AUC values produced by each method and the 'violins' are coloured according to whether the distributions significantly vary to Boosted Trees using a critical value of 0.05.

```{r reduced_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the methods when predicting profit/loss. Subsets of the data were used for Logistic Regression and Random Forests in order to provide datasets without missing values.")}

reduc.totAUC <- readRDS("reduc_df_auc.rds")

#significant difference??
index = c(1,3,4)
pvals = data.frame("method" = c('log','rf','naiive'), 'p_value' = NA)
for(i in seq_along(index)){
        pvals$p_value[i] = t.test(reduc.totAUC[[2]], reduc.totAUC[[index[i]]])$p.value %>%
                format(digits = 2) %>% as.numeric
}

signif = pvals %>% filter(p_value < 0.05) %>% .$method
reduc.totAUC = melt(reduc.totAUC)
reduc.totAUC$sigdiff = ifelse(reduc.totAUC$L1 %in% signif, TRUE, FALSE)


ggplot(reduc.totAUC, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = sigdiff, colour = sigdiff), alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", shape =3) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        scale_fill_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from Boosted Trees") +
        scale_colour_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from Boosted Trees") +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 models of Each Method") +
        scale_x_discrete(labels = c("boost" = "Boosted Trees", 
                         "log" = "Logistic Regression",
                         "naiive" = "Naive Bayes", "rf" = "Random Forest")) +
        theme(legend.position = "bottom")
        

        
```

The AUC performance of Logistic Regression and the Random Forest algorithms cannot be statistically differentiated from Boosted Trees.

Next, data imputation methods were trialed which would make the complete data set available to Logistic Regression and Random Forests. Again, 100 models were required to achieve a power of 0.8 with respect to Boosted Trees

```{r mice_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the methods when predicting profit/loss. Each method was fed the same imputed full dataset.")}


mice.totAUC <- readRDS("mice_df_auc.rds")

#significant difference??
index = c(1,3,4)
pvals = data.frame("method" = c('log','rf','naiive'), 'p_value' = NA)
for(i in seq_along(index)){
        pvals$p_value[i] = t.test(mice.totAUC[[2]], mice.totAUC[[index[i]]])$p.value %>%
                format(digits = 2) %>% as.numeric
}

signif = pvals %>% filter(p_value < 0.05) %>% .$method
mice.totAUC = melt(mice.totAUC)
mice.totAUC$sigdiff = ifelse(mice.totAUC$L1 %in% signif, TRUE, FALSE)

ggplot(mice.totAUC, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = sigdiff, colour = sigdiff), 
                    alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", shape =3) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        scale_fill_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from boosted trees") +
        scale_colour_manual(values = c('sienna1','navajowhite'),
                          name = "Significantly different from boosted trees") +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 Models of Each Method - Imputed Data") +
        scale_x_discrete(labels = c("boost" = "Boosted Trees", 
                         "log" = "Logistic Regression",
                         "naiive" = "Naive Bayes", "rf" = "Random Forest")) +
        theme(legend.position = "bottom")
```

Boosted Trees, Logistic Regression, and Random Forest all performed significantly better than the baseline algorithm, Naive Bayes, however none outperformed Boosted Trees.

Testing Boosted Trees, Random Forests, Naive Bayes, Logistic Regression and Bayesian Networks on the binary classification problem showed that Boosted Trees, Logistic Regression and Random Forests performed best (according to AUC). Naive Bayes and Logistic Regression were included as baseline models and it was expected that the more complex models would outperform these. Therefore it was surprising that results from 100 Logistic Regression models were not statistically significantly lower than Boosted Trees.

A possible explanation for this, according to the literature, is that there was not enough data for the ensemble trees to learn the complex decision rules at which they excel. Trees tend to overfit the patterns in a smaller training set. Logistic Regression on the other hand is capable of only one decision boundary (which does not have to be parallel to the variable axes) and is not prone to overfitting [@Perlich2003]. This may explain Logistic Regression's comparatively high performance on the case study's small but complex data set.

Boosted trees, Random Forests and Logistic Regression all had mean AUC values between 0.755 to 0.764. In summary, the individual binary classification models performed well above random chance (AUC = 0.5). Whether the model is worth implementing in the work place is dependent on the extent to which the algorithm would improve 'bottom line' profits for the business and if the model can affect decisions in practice.

- NB and BN were bad - one sentence

#### 4.2 Blended Models

Blended models combine the predictions from each high-performing individual model to create averaged or 'blended' predictions. In this case, the Random Forest, Boosted Trees, and Logistic Regression models were chosen. Through variable importance studies, 6 variables were included as metafeatures. All six methods were compared against the original Logistic Regression model's AUC distribution (as it was not statistically different from the ensemble trees)


```{r blend_violin, fig.height = 5.5, fig.cap = c("Violin plot vertically illustrating the distribution of AUC values from each of the blending methods when predicting profit/loss. 100 models were built for each method.")}

auc.list <- readRDS("blend_auc2.rds")
means = lapply(auc.list, mean) %>% unlist
greater = means[means> means["orig.log"]] %>% names
auc.melt = melt(auc.list)
auc.melt$greater = ifelse(auc.melt$L1 %in% greater, TRUE, FALSE)


ggplot(auc.melt, aes(x= factor(L1), y= value)) + 
        geom_violin(aes(fill = greater, colour = greater), 
                    alpha = 0.8) + 
        stat_summary(fun.y="mean", geom="point", colour = "gray30", 
                     shape =4) +
        stat_summary(aes(label = round(..y.., 3)), fun.y="mean",
                     geom="text", colour = "gray20", size =3, 
                     hjust = -.2, angle = 40, vjust = -.2
                     ) +
        theme(axis.text.x=element_text(angle=40,hjust=1)) +
        labs(x = "Method", y = "Area Under Curve (AUC)",
             title = "AUC Distribution from 100 Blended Models of each Method") +
        scale_x_discrete(labels = c("boost" = "BT Blend", 
                         "log" = "FWLS", 
                         "average" = "Simple Average Blend",
                         "orig.log" = "Orignal LR", 
                         "rf" = "RF Blend",
                         "simp.boost" = "Simple BT Blend",
                         "simp.log" = "Simple LR Blend")) +
        scale_fill_manual(values = c('navajowhite','sienna1'),
                          name = "AUC Greater than Original Logistic Regression") +
        scale_colour_manual(values = c('navajowhite','sienna1'),
                          name = "AUC Greater than Original Logistic Regression") +
        theme(legend.position = "bottom")


```

The above plot shows that 4 methods had a higher mean AUC than the original Logistic Regression model: 

* the simple blended Logistic Regression 
* the simple average 
* and the two Boosted Tree models.

The simple averaged method and simple logistic regression could achieve a statistical power of 0.8 with 150 samples (models), whereas the boosted trees required thousands. The additional 50 models were run for the simple logistic regression and simple average to reveal similar distributions that were indeed statistically significantly higher than the individual logistic regression AUC distribution. P-values from two sample t-test were 0.0019 and 0.0021 respectively.

 Blended models improved the mean AUC from 0.759 to 0.77, which is an increase of `r ((0.77-0.759)/0.759) %>% "*"(100) %>% round(1)`%. It was expected that model blending would improve predictions because it combined the strengths of individual models with different theoretical foundations.
 
 The trials of different blending methods demonstrated that again, the simplest methods worked best with the case study data. In this incident, averaging the results of the best three single models (Logistic Regression, Random Forests, and Boosted Trees) or taking a Logistic Regression of the three models outperformed more complex Boosted Trees, Random Forests, and Logistic Regression blends that facilitated interaction of the individual model results and original variables (meta-features). Simple blended models could perform better than complex methods if the data is not big enough for the complex models to learn the more intricate patterns at which they excel. This was observed in the single model methods. their success in the 2009 Netflix competition generated some publications. The Netflix data set comprised of almost 3,000,000 observations, so the size of the data could have enabled the success of complex blending methods such as FWLS [@Sill2009]


5. Business Impact
==================

This chapter first presents the full range of method results in terms of improvements to the case study's bottom line. A business decision making scenario was created so that profit curves based on the decision rule could be built and analysed. From the profit curves, optimal probability thresholds could be derived for each method (individual and blended methods). The predictive models output a 'probability' between 0 and 1 that each project will be a loss making job (where probability = 1 indicates a loss making job). The question the arises, at what probability would a decision-maker round the probability up to 1 or down to 0? And what business decision would then be made?  To find the threshold point for rounding, an experimental business-scenario was tested. At a given threshold, all projects with probability outputs above the threshold were considered too risky, and were rejected. All profits and losses from these projects were forfeited, while the profits from the remaining jobs (below the threshold) were summed to give a revised total profit. This profit calculation was made on a range of thresholds between 0 and 1 at 0.05 increments. The total profits were plotted for each threshold and joined to make a profit curve

The plot below illustrates the distribution of profit curves for each method, where the solid lines join the mean values at each threshold point. The grey ribbon illustrates the 95% confidence interval for the profit improvement ratio at each threshold point for the 100 models.

```{r profit_curve, fig.height = 6, fig.cap = c("Profit curves summarising results from 100 models of 9 methods: 3 simple blends, 3 complex blends, and the original 3 best methods.")}

finance.plot = readRDS("plot_finance_100_4.rds" )
finance.plot$method <- factor(finance.plot$method, 
                                    levels=c("average", "simp.log","simp.boost",
                                             "comp.boost","comp.rf", "FWLS","orig.log",
                                             "orig.rf","orig.boost"))

ggplot(finance.plot, aes(x= threshold, y= profit.ratio, colour = method)) + 
        # geom_point(alpha = 0.005, shape = 16, size = 1) + 
        geom_hline(yintercept = 100, colour = "darkgray", linetype = 2) + 
        geom_hline(yintercept = 0, colour = "darkgray", linetype = 1) +
        stat_summary(fun.y = mean, 
                     fun.ymin = function(x) mean(x) - 1.96*sd(x),
                     fun.ymax = function(x) mean(x) + 1.96*sd(x),
                     fill = "gray", geom= "ribbon", alpha = 0.7, colour = NA) +
        stat_summary(fun.y = mean, geom="line") +
        scale_y_continuous(breaks = seq(0,110, by = 20)) + 
        scale_x_continuous(breaks = seq(0,1, by = .2)) +  
        theme(legend.position = "none",
              panel.background = element_rect(fill = 'gray95')) +
        labs(x = "Probability Threshold", 
             y = "Ratio of Model Profit to Historical Profit (%)",
             title = "Profit Curve for Blended and Individual Models") + 
        facet_wrap(~method, 
                   labeller = labeller( method = c("average" = "Simple Average Blend",
                                       "orig.boost"= "Orig. Boosted Trees",
                                       "orig.log" = "Orig. Logistic Regression",
                                       "orig.rf" = "Orig. Random Forest",
                                       "simp.log" = "Simple Log. Reg. Blend",
                                       "simp.boost" = "Simple Boosted Tree Blend",
                                       "comp.boost"= "Complex Boosted Tree Blend",
                                       "comp.rf"= "Complex Random Forest Blend",
                                       "FWLS" = "FWLS"
                                       )))

```

Two blended models clearly outperformed the individual models as shown in the above plot with higher profit ratios as well as tighter confidence intervals. The simple Logistic Regression blend performed best with the highest mean profit ratio of 109% with a standard deviation of 1.28%. %. This means that for the simple blended Logistic Regression model, if all jobs above the probability threshold 0.6 were rejected, the profits would increase on average by 9% in comparison to historical profits.

Profit curves from the simple average blend and Logistic Regression blended models outperformed the complex blended models, which follows logically from their significantly higher AUC distributions. The simple average blended model produced a profit curve with an almost identical profit improvement (and standard deviation) to the Logistic Regression blend. In the data, projects assigned a probability higher than 0.6 represent only 4.3% of all projects.

The shaded grey 95% confidence intervals in the Profit Curve plot shows that with the 100 sample models that were analysed, none of the lower bounds for the original methods' or complex blending methods were above 100% on the y-axis. That is, it cannot be said with 95% confidence that the original methods would produce a mean profit higher than historical profits in the given decision framework. Clearly this level of certainty is not beneficial for the case-study business to adopt.

It is not clear why simple linear models and the averaging model outperformed ensemble tree blending methods. As previously stated, the ensemble trees may not have received enough data to adequately learn the complex series of rules they develop. The only differences between FWLS and the simple Logistic Regression (that performed best) were two additional explanatory variables and four interaction terms. The number of variables was not high for the amount of data since according to @Peduzzi1996, 10 events per explanatory variable or more avoids the risk of biased estimation of variable coefficients in Logistic Regression. The data contained 315 events per explanatory variable and did not pose that risk. Nevertheless, the additional variables must have added misleading noise to the model.

To conclude, because of the promising AUC results, it was logical that the models translated into financial benefits for the company. The 9% improvement in profits produced by the simple blended Logistic Regression is reasonable, and may be high enough to trigger further cost-benefit analyses and the development of a more comprehensive framework describing different decision scenarios.

- decision scenario

- profit curves

- prototype decision support interface


6. Conclusions and Future Work
==============================

3/4 page

The general aim was to use statistical techniques to predict the profitability of projects for a case study consulting business using their internal CRM data. This was rigorously completed by approaching the prediction of 'profitability' as a binary classification problem predicting either profit or loss. Several statistical and machine learning approaches were applied including Naive Bayes, Bayesian Networks, Linear Regression, Random Forests, gradient Boosted Trees as well as multiple methods of blending the individual models' output.

AUC = 0.76). This was achieved by 3 individual methods while various techniques that blended the individual methods improved results further (AUC = 0.77).

predictive models could be shown to improve the overall profitability (bottom line) of the case study business. A range of probability threshold values were trialed for each method using a decision framework where projects scored below the threshold were accepted, while projects above the threshold were rejected. If a project was rejected, the profits and losses were forfeited, and the remaining accepted profits and losses were summed.

Final results showed the simple Logistic Regression blend of the individual Logistic Regression, Random Forest and Boosted Tree models improved profits the most. The 95% confidence interval for these improvements was between 6.5% and 11.5% using a probability threshold of 0.6 (approximately 4.3% of projects).

These results contribute significantly to the research in cost estimation in three ways: the applied methods, the decision framework, and the appeal to user trust. Ensemble tree methods and blending had been applied minimally to cost estimation previously, even though ensemble trees provide insight into model structure while predicting at a similar level to Neural Networks. Next, previous studies have verified predictive accuracy but stopped short of how the algorithm would affect decisions and what the measured benefits would be. This study presented a clear framework for how a business could improve profits by applying the algorithm.

Further work is required to test user confidence in the output. Another topic identified for future research was to test how well managers estimate some numeric project input variables. In particular, time span, team size, total invoiced amount, and percentage of hours completed by professionals should be tested as these variables were calculated post project completion. Time span and total invoiced amount were discretised into wide categories, which should be easier for a manager to choose between.

future work: user interface.

Overall, this work has successfully built a mathematical blend of Logistic Regression, Random Forests, and Boosted Tree models, from a consulting company's internal project data. This blended model can predict whether a project will be profitable or not and in a reasonable decision framework, can guide managers in rejecting financially risky projects and improving profitability of the business.

Acknowledgments
===============

This research was supported by a scholarship/ ACEMS?

References {#references .unnumbered}
==========

#### Installation

If the document class *elsarticle* is not available on your computer,
you can download and install the system package *texlive-publishers*
(Linux) or install the LaTeX package *elsarticle* using the package
manager of your TeX installation, which is typically TeX Live or MikTeX.

The author names and affiliations could be formatted in two ways:

(1) Group the authors per affiliation.

(2) Use footnotes to indicate the affiliations.

Bullet points.

-   document style

-   baselineskip

-   front matter

-   keywords and MSC codes

