---
title: "Appendix"
output: word_document
---

```{r, echo=FALSE, include=FALSE}

library('knitr')
library('ggplot2')
library("plyr")
library("dplyr")
library('magrittr')
library('reshape2')
library("rpart")
library('car')
library('e1071')
library('party')
library('randomForest')
library('RColorBrewer')
library('pwr')
library('scales')
library('caret')
library('pROC')
library('gbm')


# detect OS
if(.Platform$OS.type == 'windows'){
        # setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
        opts_knit$set(root.dir= "C:/Users/n9232371/Documents/github/consult/finalwriteup/report_data")
} else{
        # setwd("~/OneDrive/shared files/Bligh Tanner/masters/data")
        # setwd("~/Documents/github/consult/finalwriteup/report_data")
        opts_knit$set(root.dir= '~/Documents/github/consult/finalwriteup/report_data')
}

opts_chunk$set(fig.width=6.7, dpi = 300, warning = FALSE, message = FALSE, echo = FALSE, warning = FALSE)
```

# Appendix

Example results from:

## Regression models

###Linear regression

Example summary of model built with core variables only:
```{r}

all8a = read.csv("report_data/all8a.csv")[,-1]

#function below outputs summary of model
final.output<- function(df=all8a, no.tests = 50, core = TRUE) {
        
        summary.tests = matrix(NA, nrow = no.tests, ncol = 3)
        
        for(i in 1:no.tests){
        
        if(core == FALSE){
        samp = sample.int(nrow(df), 1)                        
        #create sample df - use all variables that are complete from a sample row of data
        sample = df[samp,]
        sample = sample[,!is.na(sample)]
        vars= names(sample)
        vars = vars[! vars %in% c('mlsto','Year','return.pdol')]
        variables = vars
        } else{
                variables = c('inv.mlsto.log', 'Discipline', 'timespan.cbrt',
                              'no.users', 'pc.pro', 'Business')
        }


        #make reduced dataset with complete cases for all variables we want to use
        reduced = df[complete.cases(df[,variables]),]
        
        formula = paste("return.pdol ~ ")
        
        for(j in 1:length(variables)){
                formula = paste(formula, variables[j], sep= " + ")
        }
        
        if('inv.mlsto.log' %in% variables &
           'pc.pro' %in% variables){
                formula = paste(formula, 'inv.mlsto.log*pc.pro', sep= " + ")
        }
        
        if('inv.mlsto.log' %in% variables &
           'code.client' %in% variables){
                formula = paste(formula, 'inv.mlsto.log*code.client', sep= " + ")
        }
        
        if('JD.Second' %in% variables &
           'no.users' %in% variables){
                formula = paste(formula, 'JD.Second*no.users', sep= " + ")
        }
        
        samp_size = floor(0.75*nrow(reduced))
        create.train = base::sample.int(nrow(reduced), samp_size)
        train.df = reduced[create.train,]
        test.df = reduced[-create.train,]

        cats = c('code.client', 'code.contact', 'JD.Second', 'Business', 'majority.pos')
        for(k in seq_along(cats)){
                if(cats[k] %in% variables){
                test.df = test.df[test.df[,cats[k]] %in% unique(train.df[,cats[k]]),]
        }
        }

        final = aov(as.formula(formula), data = train.df)
        
        fit= summary(final)
        
        }

        return(fit)
}

final.output(df=all8a, no.tests = 1, core = TRUE)

```

Example summary of model built with all variables:

```{r}

final.output(df=all8a, no.tests = 1, core = FALSE)

```


### Random Forest

Example summary of model built with core variables only:

```{r}

final.rf.output<- function(df=all8a, no.tests = 50, core = TRUE) {
        
        summary.tests = matrix(NA, nrow = no.tests, ncol = 3)
        
        for(i in 1:no.tests){
        
        if(core == FALSE){
        samp = sample.int(nrow(df), 1)                        
        #create sample df - use all variables that are complete from a sample row of data
        sample = df[samp,]
        sample = sample[,!is.na(sample)]
        vars= names(sample)
        vars = vars[! vars %in% c('mlsto','Year','return.pdol')]
        variables = vars
        } else{
                variables = c('inv.mlsto.log', 'Discipline', 'timespan.cbrt',
                              'no.users', 'pc.pro', 'Business')
        }


        #make reduced dataset with complete cases for all variables we want to use
        reduced = df[complete.cases(df[,variables]),]
        
        if('code.client' %in% variables){
        sample = factor(sample(unique(reduced$code.client),size=53,
                                     replace=FALSE))
        reduced= reduced[reduced$code.client %in% sample,]

        reduced$code.client= factor(reduced$code.client)
        }
        
        if('code.contact' %in% variables){
        sample1 = factor(sample(unique(reduced$code.contact),size=53,
                                     replace=FALSE))
        reduced= reduced[reduced$code.contact %in% sample1,]

        reduced$code.contact= factor(reduced$code.contact)
        }
        
        formula = paste("return.pdol ~ ")
        
        for(j in 1:length(variables)){
                formula = paste(formula, variables[j], sep= " + ")
        }
        
        
        samp_size = floor(0.75*nrow(reduced))
        create.train = base::sample.int(nrow(reduced), samp_size)
        train.df = reduced[create.train,]
        test.df = reduced[-create.train,]

        cats = c('code.client', 'code.contact', 'JD.Second', 'Business', 'majority.pos')
        for(k in seq_along(cats)){
                if(cats[k] %in% variables){
                test.df = test.df[test.df[,cats[k]] %in% unique(train.df[,cats[k]]),]
        }
        }

        final = randomForest(as.formula(formula), data= train.df, mtry=5,ntree=500, importance=FALSE)
        

        
        }
        return(final)
}

rf.core <- final.rf.output(df=all8a, no.tests = 1, core = TRUE)

print(rf.core)
getTree(rf.core, 1) %>% head(20)
```


All variables:

```{r}
rf.all <- final.rf.output(df=all8a, no.tests = 1, core = FALSE)

print(rf.all)
getTree(rf.all, 1) %>% head(20)

```

## Classification models

### Logistic Regression

Example summary output imputed data:

```{r}

# load imputed data
all10mice_f = read.csv("report_data/all10mice_May16.csv")[,-1]


#function for running models
test.roc <- function(dfs = c('all10mice_f'), seed = 100, mtrys = c(5), method = "log",
                     extra.var = NULL, mult = 1, x.folds = 10){
        
        auc<- rep(list(rep(0, length(dfs))), length(dfs))
        colours <- topo.colors(x.folds*mult)


        for(i in 1:length(dfs)){
                df = get(dfs[i])
                
                # mlsto is spinning errors, get rid of
                df = df[, !(names(df) %in% 'mlsto')]
                set.seed(seed)
                
                # create f.rpdol
                df$f.rpdol <- as.factor(df$b.rpdol)
                levels(df$f.rpdol)[levels(df$f.rpdol) == "0"] <- "profit"
                levels(df$f.rpdol)[levels(df$f.rpdol) == "1"] <- "loss"
                
                #set up 10 fold cv using caret function
                folds <- createFolds(df$f.rpdol, k = x.folds, list = T, returnTrain = F)
                #now have folds[[1]] through folds[[10]] list of rows to exclude
                
                #for Appendix just want one fold
                folds <- folds[[1]]
                
                if(mult > 1){
                        for(k in 2:mult){
                                set.seed(sample(1:1000,1))
                                folds.temp = createFolds(df$f.rpdol, k = x.folds, list = T, returnTrain = F)
                                folds = append(folds, folds.temp)
                        }
                }
                
                
                
                #if extra.var not found in names(df), change to NULL
                if(!is.null(extra.var)){
                        if(!(extra.var %in% names(df))) extra.var = NULL
                }

                
                #define formula for algorith methods, either rf, log, boost
                
                if(nrow(df) == 2364){
                        if(is.null(extra.var)){
                                formula <-"f.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + b.inv.log + client.totinv.log + Business + majority.pos + pc.majpos.log + JD.Second + Billing.Type"
                                
                        }}
                
                if(method %in% 'boost') {
                        #no JD.Second
                        formula <-"b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + b.inv.log + client.totinv.log + Business + majority.pos + pc.majpos.log"

                }
                
                if(method %in% 'naiive') {
                        formula<- "b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + inv.mlsto.log + client.totinv.log + Business + majority.pos + JD.Second + pc.majpos.log + Billing.Type + JD.Second"

                }
                
                        
                        #turn predictors and response into 'ordered' variables for roc() to work
                        test<- df[folds,]
                        levels(test$f.rpdol) <- c('profit', 'loss')
                        test$f.rpdol <- ordered(test$f.rpdol)
                        
                        
                        if(method %in% 'rf'){
                                fit <- randomForest(as.formula(formula), data = df[-folds,], 
                                            mtry = mtrys[i], ntree = 1000)
                                pred <- predict(fit, test, type = "prob")
                                pred <- pred[,2]
                                
                        }
                        
                        if(method %in% 'log'){
                                fit<- glm(as.formula(formula), 
                                          data = df[-folds,], family = binomial())
                                pred <- predict(fit, test,
                                                type = "response")
                        } 
                        if(method %in% 'boost'){
                                fit <- gbm(as.formula(formula), data = df[-folds,],
                                           distribution = "bernoulli", n.trees = 10000,
                                           shrinkage = 0.001, interaction.depth = 5,
                                           n.minobsinnode = 20)
                                
                                pred <- predict(fit, test, n.tree = 10000, type = "response")
                        }
                        
                        if(method %in% 'naiive'){
                                fit<- naiveBayes( as.formula(formula), df[-folds,])
                                pred <- predict(fit, test, type = 'raw')
                                pred<- pred[,2]
                        }

                        
                        #pROC
                        pred.p <- pROC::roc(test$f.rpdol, pred)
                        
                        if (i == 1) {
                        plot.roc(pred.p, col = colours[i], print.thres = F)
                        
                        } else{
                        plot.roc(
                        pred.p, col = colours, add = TRUE , print.thres = F)
                        }
                        
                        auc<- auc(pred.p)
                        names(auc) = method
                        
                        

                
        }
        
        assign("auc.func", auc, envir = .GlobalEnv)
        assign("formula", formula, envir = .GlobalEnv)
        return(fit)
       
}

Log.Reg <- test.roc(dfs = c('all10mice_f'), seed = 100, mtrys = c(5), method = "log",
                     extra.var = NULL, mult = 1, x.folds = 10)
auc.func

summary(Log.Reg)

```

### Random Forest

```{r}

Rand.F <- test.roc(dfs = c('all10mice_f'), seed = 100, mtrys = c(5), method = "rf",
                     extra.var = NULL, mult = 1, x.folds = 10)
auc.func

Rand.F
summary(Rand.F)

```

### Boosted Trees

```{r}

Boost.T <- test.roc(dfs = c('all10mice_f'), seed = 100, mtrys = c(5), method = "boost",
                     extra.var = NULL, mult = 1, x.folds = 10)
auc.func

# Summary of the first tree in the boosted tree
pretty.gbm.tree(Boost.T) %>% kable

summary(Boost.T)

```


### Naive Bayes

```{r}

Naive <- test.roc(dfs = c('all10mice_f'), seed = 100, mtrys = c(5), method = "naiive",
                     extra.var = NULL, mult = 1, x.folds = 10)
auc.func

Naive

```

### Bayesian Network


## Blended Models


### Logistic Regression

#### Simple Blend

```{r}

if(.Platform$OS.type == 'windows'){

        source("C:/Users/n9232371/Documents/github/consult/functions.R")
} else{

        source("/Users/yam/Documents/github/consult/functions.R")

}

df.blend <- readRDS("report_data/dfblend.rds")

iter.auc <- function(mult =1, x.folds.iter = 5, 
                     methods = c("simp.log"),
                     df.iter = df.blend) {
        
        #create test and train set: 0.75/0.25
        pred.list <- vector(mode = "list", length = length(methods))
        names(pred.list) = methods
        
        #create model averaged column
        if("average" %in% methods){
        df.iter = mutate(df.iter, model.av = (log + rf + boost)/3) 
        }
        
        #set up 10 fold cv using caret function
        folds <- createFolds(df.iter$f.rpdol, k = x.folds.iter, list = T, returnTrain = F)
        #now have folds[[1]] through folds[[10]] list of rows to exclude
        folds <- folds[[1]]
        
                train = df.iter[-folds,]
                test = df.iter[folds,]
                
                if ("simp.log" %in% methods) {
                        print("simp.log")
                        fit <- glm(f.rpdol ~ log + rf + boost,
                                   data = train,
                                   family = binomial())
                        
                        pred <- predict(fit, test, type = "response")
                        pred.list[["simp.log"]] = rbind(pred.list[["simp.log"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                        
                }
                
                if ("simp.boost" %in% methods) {
                        print("simp.boost")
                        fit <- gbm(b.rpdol ~ norm.log + norm.rf + norm.boost, data = train,
                                   distribution = "bernoulli", n.trees = 6000,
                                   shrinkage = 0.0005, interaction.depth = 6,
                                   n.minobsinnode = 10)
                        
                        pred <- predict(fit, test, n.tree = 6000, type = "response")
                        pred.list[["simp.boost"]] = rbind(pred.list[["simp.boost"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                        
                }

                
                if ("orig.log" %in% methods) { 
                        print("orig.log")
                        pred <- test$log
                        pred.list[["orig.log"]] = rbind(pred.list[["orig.log"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                        
                }
                
                if ("orig.rf" %in% methods) { 
                        print("orig.rf")
                        pred <- test$rf
                        pred.list[["orig.rf"]] = rbind(pred.list[["orig.rf"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                        
                }
                
                if ("orig.boost" %in% methods) { 
                        print("orig.boost")
                        pred <- test$boost
                        pred.list[["orig.boost"]] = rbind(pred.list[["orig.boost"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                        
                }
                
                if ("average" %in% methods) {
                        print("average")
                        pred <- test$model.av
                        pred.list[["average"]] = rbind(pred.list[["average"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                }
                
                if ("FWLS" %in% methods) {
                        print("FWLS")
                        fit <- glm(f.rpdol ~ norm.log + norm.rf + norm.boost + b.timespan.cbrt*norm.log + b.inv.log*norm.log + b.inv.log*norm.rf + Business*norm.boost,
                                   data = train,
                                   family = binomial())
                        pred <- predict(fit, test, type = "response")
                        pred.list[["FWLS"]] = rbind(pred.list[["FWLS"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                }
                
                if ("comp.boost" %in% methods) {
                        print("comp.boost")
                        fit <- gbm(b.rpdol ~ b.timespan.cbrt + no.users + b.inv.log + Business + JD.Second + norm.log + norm.rf + norm.boost, 
                                   data = train,
                                   distribution = "bernoulli", n.trees = 6000,
                                   shrinkage = 0.0005, interaction.depth = 3,
                                   n.minobsinnode = 10)
                        
                        pred <- predict(fit, test, n.tree = 6000, type = "response")
                        pred.list[["comp.boost"]] = rbind(pred.list[["comp.boost"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                }
                
                if ("comp.rf" %in% methods) {
                        print("comp.rf")
                        fit <- randomForest(f.rpdol ~ b.timespan.cbrt + no.users + b.inv.log + Business + JD.Second + norm.log + norm.rf + norm.boost,
                                            data = train, mtry = 3, 
                                            ntree = 1000, importance = TRUE
                        )
                        pred <- predict(fit, test, type = "prob")
                        pred <- pred[, 2]
                        pred.list[["comp.rf"]] = rbind(pred.list[["comp.rf"]],
                                                        data.frame("index" = folds,
                                                                   "pred" = pred,
                                                                   "iteration" = paste("iter",ceiling(k/x.folds.iter),m, sep = "")))
                }
                
                
        #pROC
                        pred.p <- pROC::roc(test$f.rpdol, pred)
                        
                        plot.roc(pred.p, col = colours[i], print.thres = F)
                        
                        auc<- auc(pred.p)  
                        assign("auc.func", auc, envir = .GlobalEnv)

        
        return(fit)
        
}

simp.LR <- iter.auc(mult =1, x.folds.iter = 5, 
                     methods = c("simp.log"),
                     df.iter = df.blend)
auc.func
summary(simp.LR)


```

#### Complex Logistic Regression Blend

```{r}
complex.LR <- iter.auc(mult =1, x.folds.iter = 5, 
                     methods = c("FWLS"),
                     df.iter = df.blend)
auc.func
summary(complex.LR)

```

### Boosted Trees 

#### Simple Blend

```{r}
simp.boost <- iter.auc(mult =1, x.folds.iter = 5, 
                     methods = c("simp.boost"),
                     df.iter = df.blend)
auc.func
summary(simp.boost)

# Summary of the first tree in the boosted tree
pretty.gbm.tree(simp.boost) %>% kable

```

#### Complex Blend

```{r}
comp.boost <- iter.auc(mult =1, x.folds.iter = 5, 
                     methods = c("comp.boost"),
                     df.iter = df.blend)
auc.func
summary(comp.boost)

# Summary of the first tree in the boosted tree
pretty.gbm.tree(simp.boost) %>% kable

```

### Random Forest Complex Blend

```{r}
comp.rf <- iter.auc(mult =1, x.folds.iter = 5, 
                     methods = c("comp.rf"),
                     df.iter = df.blend)
auc.func
comp.rf
summary(comp.rf)


```



