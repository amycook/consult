---
title: "Boosted"
author: "Amy Cook"
date: "October 19, 2015"
output: word_document
---


```{r, load library and packages, include= FALSE}
library("plyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library("dplyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library('magrittr',lib='C:/Progra~1/R/R-3.2.1/library')
library('reshape2',lib='C:/Progra~1/R/R-3.2.1/library')
library('ggplot2',lib='C:/Progra~1/R/R-3.2.1/library')
library('gbm', lib = 'C:/Progra~1/R/R-3.2.2/library')
library('caret', lib = 'C:/Progra~1/R/R-3.2.2/library')
library('pROC',lib='C:/Progra~1/R/R-3.2.2/library')

# setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
```

Read in data. Change return.pdol to 0,1 variable

```{r, echo=FALSE}
all8a<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all8a.csv')[,-1]

all8a$b.rpdol<- all8a$return.pdol 
all8a$b.rpdol<- ifelse(all8a$return.pdol<=0, 1, 0)
all8a$pc.pro<- ifelse(all8a$pc.pro == 0 & is.na(all8a$pc.majpos.log), NA, all8a$pc.pro)
all8a$timespan.cbrt <- ifelse(all8a$timespan.cbrt ==0, 1, all8a$timespan.cbrt)

summary(all8a$b.rpdol)

all9a<- all8a
write.csv(all9a, 'C:/Users/n9232371/Documents/Consultbusiness/data/all9a.csv')
all9a<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all9a.csv')[,-1]

```


Create test and train sample

```{r, echo=FALSE}
set.seed(200)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

```

Try building a boosted tree wihtout any training

```{r, echo=FALSE}
boost.1 <- gbm(b.rpdol~.- mlsto - return.pdol-Year-code.contact - code.client, 
               distribution = "bernoulli", data=train,
          n.trees = 10000,
          shrinkage = 0.001,
          interaction.depth = 1,
          n.minobsinnode = 10,
          bag.fraction = 0.5)

print(boost.1)
summary(boost.1)

#look at first tree
pretty.gbm.tree(boost.1,9000)
boost.1$var.names

#view OOB error plot
gbm.perf(boost.1, method="OOB", plot.it=TRUE)
#error still rapidly reducing at 10000 trees

#partial dependence plots:
i=8
a<- plot.gbm(boost.1, i, type="response")
a
#Discipline, Business, inv.mlsto.log, client.totinv.log, Billing.Type dont matter?


```

First tune shrinkage and plot. Get rid of year because in future, won't have historical data on year..

Try deleting code.client variable

```{r, echo=FALSE}

#tune which variables should be included!!
vars.excl = c("JD.Second", "Business", "JD.Second-Business","")

errors<- double(4)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
auc<- double(4)
colours <- c("red","green","blue", "pink")
        
for(i in 1:4){
        if(vars.excl[i] %in% ""){
              formula<- "b.rpdol~.- mlsto - return.pdol-Year - code.contact - code.client"  
        } else{
        formula<- paste("b.rpdol~.- mlsto - return.pdol-Year-code.client - code.contact",
                        "-", vars.excl[i], sep=" ")
        }
        
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees=5000,
                 shrinkage=0.001,
                 interaction.depth=3)
        pred<- predict(fit, test, n.tree=5000, type="response")
        errors[i]<- sum(temp)/nrow(test)
        
        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres=T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE, print.thres=T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred1' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred1, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=shrink.seq, "auc" = auc[1:4])
ggplot(shrink, aes(x=shrinkage, y= auc)) + geom_line() + geom_point()+ geom_line()

#best to not have JD.Second OR Business!!!  :(
```

Tune shrinkage value:
```{r, echo=FALSE}

formula<- "b.rpdol~.- mlsto - return.pdol-Year-code.client - code.contact-JD.Second - Business"

errors<- double(4)
shrink.seq<-c(0.0005, .001,.005, 0.01)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
auc<- double(4)
colours <- c("red", "yellow", "green", "blue")
        
for(i in 1:4){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees=5000,
                 shrinkage=shrink.seq[i],interaction.depth=3)
        pred<- predict(fit, test, n.tree=5000, type="response")
        errors[i]<- sum(temp)/nrow(test)
        
        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres=T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE, print.thres=T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred1' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred1, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=shrink.seq, "auc" = auc[1:4])
ggplot(shrink, aes(x=shrinkage, y= auc)) + geom_line() + geom_point()+ geom_line()

#best case auc = 0.756 with shrinkage = 0.001

```

```{r, echo=FALSE}


#shrinkage should be 0.001, now find number of trees

ntree.seq<-c(2000,3000,4000,6000,8000)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)))
auc<- double(5)
colours <- c("red", "yellow", "green", "blue", "purple")

for(i in 1:5){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= ntree.seq[i],
                 shrinkage=0.001,interaction.depth=3)
        pred<- predict(fit, test, n.tree=ntree.seq[i], type="response")
        errors[i]<- sum(temp)/nrow(test)
        
        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres=T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE, print.thres=T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=ntree.seq, "auc" = auc[1:5])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

#6000 trees seems best?

#----------------------------------------------------------------------

#shrinkage should be 0.001, 4000 trees, now find interaction.depth

int.seq<-c(1,2,3,4,6)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)))
auc<- double(5)
colours <- c("red", "yellow", "green", "blue", "purple")

for(i in 1:5){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 6000,
                 shrinkage=0.001,interaction.depth=int.seq[i])
        pred<- predict(fit, test, n.tree=6000, type="response")
        
        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres=T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE, print.thres=T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=int.seq, "auc" = auc[1:5])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

## use interaction.depth = 3



#------------------------------------------------
# now tune min obs in node

obs.seq<-c(10,20,40,50)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
auc<- double(4)
colours <- c("red", "yellow", "green", "blue")

for(i in 1:4){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 6000,
                 shrinkage=0.001,interaction.depth=3,
                 n.minobsinnode = obs.seq[i])
        pred<- predict(fit, test, n.tree=6000, type="response")

        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres = T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=obs.seq, "auc" = auc[1:4])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

#go with 10????!

```

So paramters are .... 

    * shrinkage = 0.001
    * exclude vars = JD.Second, Business, code.client, Year, code.contact
    * n.trees = 4000
    * interaction.depth = 3
    * minobsinnode = 10
    
to obtain AUC = 0.785

Can we change timespan to discretised without affecting AUC??


```{r, echo=FALSE}

formula<- "b.rpdol~.- mlsto - return.pdol-Year-code.client - code.contact-JD.Second - Business - timespan.cbrt"

conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
auc<- double(4)
colours <- c("red", "yellow", "green", "blue")
timeclust.seq<- c(4:7)

x<- dist(all9a$timespan.cbrt, method = 'euclidean')
h.fit<- hclust(x, method = 'ward.D2')

for(i in 1:4){
        #create timespan cluster number column
        all9a$b.timespan.cbrt<- as.factor(cutree(h.fit, k= timeclust.seq[i]))
        
        set.seed(300)
        sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
        train<- all9a[sample,]
        test<- all9a[-sample,]
        
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 4000,
                 shrinkage=0.001,interaction.depth=3,
                 n.minobsinnode = 10)
        pred<- predict(fit, test, n.tree=4000, type="response")

        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres = T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"= timeclust.seq, "auc" = auc[1:4])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

#takes it down to 0.781 with 5 clusters, not too bad for the ability to use timespan clusters.
#takes it down to 0.783 with 6 clusters

```

6 timespan clusters seems to give quite good performance :) 0.783. Lets see what those clusters are:

```{r, echo=FALSE}

#timespan.cbrt
first.vec<- all9a$timespan.cbrt
x<- dist(first.vec, method = 'euclidean')
fit<- hclust(x, method = 'ward.D2')
plot(fit)
k.choose=6
rect.hclust(fit, k = k.choose)

#assign a cluster number to all rows in data frame
all9a$b.timespan.cbrt<- as.factor(cutree(fit, k= k.choose))

for(i in 1:k.choose){
        print(
        summary(all9a %>% select(b.timespan.cbrt, timespan.cbrt) %>% 
                        filter(b.timespan.cbrt ==i))
        )
}

#clusters roughly break down to 1 day, 3 wks, 2.5m, 9m, 1.5yr, 3 yr, over 3 yr
# in cbrt days this is equivalent to breaks of 0,1,2,7589, 4.2358, 6.5, 8.183, 10.307, 14.1
#make new clusters:
all9a$b.timespan.cbrt<- cut(all9a$timespan.cbrt, 
                            breaks= c(0,2.7589, 4.2358, 6.5, 8.183, 10.307, 14.1), 
                            include.lowest=TRUE, 
                            labels= c("1d-3wk","3wk-2.5m", "2.5m-9m","9m-1.5y","1.5y-3y",">3y"))

set.seed(300)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

fit<-gbm(as.formula(formula), 
        data=train, distribution="bernoulli", n.trees= 4000,
        shrinkage=0.001,interaction.depth=3,
        n.minobsinnode = 10)
pred<- predict(fit, test, n.tree=4000, type="response")

#pROC
pred.p<- roc(test$b.rpdol, pred)
plot.roc(pred.p, col= "pink", print.thres = T)
auc(pred.p)

#reduces AUC to 0.780 :(
#go through tuning process again?

all9a<- all9a %>% select(-timespan.cbrt)
```


Attempt at caret package with gbm

```{r, echo=FALSE}

# must turn outcome into two level factor
all9a<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all9a.csv')[,-1]
all9a$f.rpdol<- as.factor(all9a$b.rpdol)
set.seed(100)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

# set seeds
set.seed(123)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
## For the last model:
seeds[[51]] <- sample.int(1000, 1)

# first tune shrinkage and number of trees
train2<- train
train2$b.rpdol<- as.factor(train2$b.rpdol)
levels(train2$b.rpdol)[levels(train2$b.rpdol)=="0"] <- "profit"
levels(train2$b.rpdol)[levels(train2$b.rpdol)=="1"] <- "loss"

formula<- "b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + inv.mlsto.log +
client.totinv.log + pc.majpos.log + Billing.Type + majority.pos"

cv.Control <- trainControl(method = "cv",
                           number = 10,
                           seeds= seeds
                           , summaryFunction=twoClassSummary,
                           classProbs=TRUE
                           )
gbmGrid <- expand.grid(shrinkage = c(0.001),
                       n.trees = c(10000), interaction.depth = c(1,2,3,4,5),
                       n.minobsinnode = c(10,20,30))
set.seed(2)
gbmFit <- train(as.formula(formula), data= train2,
                 method = "gbm", trControl = cv.Control, verbose = FALSE,
                 bag.fraction = 0.5, tuneGrid = gbmGrid,
                metric = 'ROC',
                na.action = na.pass)

plot(gbmFit)
plot(gbmFit, plotType = 'level')

#conclusion:
# min num in terminal node = 10
# max tree depth = 4
# n.trees = 10000
# shrinkage = 0.001

```

After caret tuning, find AUC for test data set for set.seed = 100

```{r, echo = FALSE}
set.seed(300)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

formula<- "b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + inv.mlsto.log + client.totinv.log + pc.majpos.log + Billing.Type + majority.pos"

fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 10000,
                 shrinkage=0.001,interaction.depth=4,
                 n.minobsinnode = 10)
pred<- predict(fit, test, n.tree=10000, type="response")

#pROC
pred.p<- roc(test$b.rpdol, pred)
plot.roc(pred.p, col= colours[i], print.thres = T)


#auc = 0.7788

```


But can we improve the AUC by bringing back some of the missing variables in a smart way?? 

For example, bring back code.client, but only if clients have more than say 10 jobs?

```{r, echo=FALSE}

#new variable for clients with more than 10 jobs
#list of clients with more than 10 jobs
all9b<- all9a
client.count<- ddply(all9b, .(code.client), nrow)
client.count<- arrange(client.count, -V1)

client.count2<- client.count[client.count$V1>20 & !is.na(client.count$code.client),]

all9b$code.client<- as.character(all9b$code.client)
all9b$reduc.client<- ifelse(all9b$code.client %in% client.count2$code.client,
                            all9b$code.client,
                            NA)
all9b$code.client<- as.factor(all9b$code.client)
all9b$reduc.client<- as.factor(all9b$reduc.client)

#see if ROC improves

formula<- "b.rpdol~.- mlsto - return.pdol- Year- code.client - code.contact- JD.Second - Business-timespan.cbrt"

client.lim <- c(20,30,40,60,70, 90)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)), '6' = data.frame(c(0,0), c(0,0)))
auc<- rep(list(rep(0,6)),6)
colours <- c("red", "yellow", "green", "blue", "purple", "pink")


#set up train and test
        set.seed = 100
        sample<- sample(1:nrow(all9a), .75*nrow(all9a), replace=F)
        train<- all9a[sample,]
        test<- all9a[-sample,]
        #set up 10 fold cv using caret function
        folds <- createFolds(train$b.rpdol, k=10, list=T, returnTrain=F)
        #now have folds[[1]] through folds[[5]] list of rows to exclude 
        
for(i in 1:6){
        
        client.count2<- client.count[client.count$V1> client.lim[i] &
                                             !is.na(client.count$code.client),]
        all9b$code.client<- as.character(all9b$code.client)
        all9b$reduc.client<- ifelse(all9b$code.client %in% client.count2$code.client,
                            all9b$code.client,
                            NA)
        all9b$code.client<- as.factor(all9b$code.client)
        all9b$reduc.client<- as.factor(all9b$reduc.client)
        

        #see if ROC improves
        
        for (j in 1:10){
                #make boosted tree
                fit<-gbm(as.formula(formula), 
                         data=train[-folds[[j]],], distribution="bernoulli", n.trees= 10000,
                         shrinkage=0.001,interaction.depth=4,
                         n.minobsinnode = 10)
                
                pred<- predict(fit, train[folds[[j]], ], n.tree=10000, type="response")
                
                        #pROC
                        pred.p<- roc(train[folds[[j]],'b.rpdol'], pred)
        
                        if(i==1 & j==1){
                                plot.roc(pred.p, col= colours[i], print.thres = F)
                
                        } else{
                                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = F)
                        }
        
                        auc[[i]][j]<- auc(pred.p)
        
                        #confusion matrix
#                         conf.mat<- coords(pred.p, "best")
#                         pred1<- ifelse(pred>= conf.mat[1], 1, 0)
#                         temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
#                         conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
#                         cat(i," ")  
                        cat(j, " ")
        }
        
        
        print(i)        
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=client.lim, "auc" = auc[[1:5]][6])
means<- sapply(auc, function(x) tail(x,1))
ggplot(shrink, aes(x=shrinkage, y=means)) + geom_line() + geom_point()
a<- ggplot(melt(auc), aes(x= factor(L1), y= value)) + geom_boxplot()


#save AUC for each 10 fold cross validation iteration, where client count cut off varied from 
client.cutoff<- melt(auc)
client.cutoff$L1<- as.factor(client.cutoff$L1)
levels(client.cutoff$L1)<- c("20","30","40","60","70", "90")
colnames(client.cutoff)[names(client.cutoff) %in% 'L1']<-'min.client.count'
colnames(client.cutoff)[names(client.cutoff) %in% 'value']<-'AUC'
write.csv(client.cutoff, 'C:/Users/n9232371/Documents/github/consult/finalwriteup/images/boosted tree/client_cutoff.csv')

a<- ggplot(client.cutoff, aes(x= min.client.count, y= AUC)) + geom_boxplot()
ggsave('C:/Users/n9232371/Documents/github/consult/finalwriteup/images/boosted tree/clientcode.jpg', a)

#client id useless for boosted trees!!

```


What about JD.Second?

```{r,echo=FALSE}

#compact JD.Second into less categories
all9a$JD.Second<- as.character(all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("heritage|expert", all9a$JD.Second), 
                          "heritage_expert", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("wharf|bridges", all9a$JD.Second), 
                          "wharf_bridge", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("membrane|art|awning|facade", all9a$JD.Second), 
                          "art_facade_awn_memb", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("hosp|emerg|health|carpark", all9a$JD.Second), 
                          "hosp_health_carpark", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("sign|product", all9a$JD.Second), 
                          "sign_product", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("hotel|office|dining", all9a$JD.Second), 
                          "hotel_office_dining", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("report|investigation", all9a$JD.Second), 
                          "report", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("waste water|management", all9a$JD.Second), 
                          "waste_wat_manage", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("univ|commun|school|exten", all9a$JD.Second), 
                          "edu_exten_community", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("multi|house", all9a$JD.Second), 
                          "residential", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("flood|harvest|recycled", all9a$JD.Second), 
                          "flood_h2o_harvest", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("bldg|subdiv|sewer", all9a$JD.Second), 
                          "civBldg_subdiv_sewer", all9a$JD.Second)
all9a$JD.Second <- ifelse(grepl("aged|erosion", all9a$JD.Second), 
                          NA, all9a$JD.Second)

all9a$JD.Second<- as.factor(all9a$JD.Second)


#now see if improvement with JD.Second

var.seq<-c("no.users + JD.Second", 
           "no.users")

conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)))
auc<- double(2)
colours <- c("red", "yellow")

#make fresh random sample of data
set.seed(1000)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

for(i in 1:2){
        
        formula<- paste("b.rpdol~ Discipline + pc.pro + inv.mlsto.log + pc.majpos.log + majority.pos + b.timespan.cbrt + client.totinv.log + Billing.Type", var.seq[i], sep= " + ")
        
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 10000,
                 shrinkage=0.001,interaction.depth=4,
                 n.minobsinnode = 10)
        pred<- predict(fit, test, n.tree=10000, type="response")

        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres = T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=1:2, "auc" = auc[1:2])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

# generally slightly better without JD.Second :(


```

What about Business?

```{r,echo=FALSE}

#compact JD.Second into less categories

all9a$Business<- as.character(all9a$Business)
all9a$Business <- ifelse(grepl("lawyer|body", all9a$Business), 
                     "lawyer_bodyC", all9a$Business)
all9a$Business <- ifelse(grepl("univ|school", all9a$Business), 
                     "uni_school", all9a$Business)
all9a$Business <- ifelse(grepl("sign|manuf", all9a$Business), 
                     "sign_manufac", all9a$Business)
all9a$Business <- ifelse(grepl("environ|water", all9a$Business), 
                     "enviro_water", all9a$Business)
all9a$Business <- ifelse(grepl("road|gov|util", all9a$Business), 
                     "road_rail_gov_util", all9a$Business)
all9a$Business <- ifelse(grepl("artist|landscape", all9a$Business), 
                     "artist_landarch", all9a$Business)
all9a$Business <- ifelse(grepl("hospit|institut|health", all9a$Business), 
                     "hosp_institut_health", all9a$Business)
all9a$Business <- ifelse(grepl("business|building|internal", all9a$Business), 
                     "biz_bldgserv_internal", all9a$Business)

all9a$Business<- as.factor(all9a$Business)

write.csv(all9a, 'C:/Users/n9232371/Documents/Consultbusiness/data/all9c.csv') # c for compacted variables
all9c<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all9c.csv')[,-1]
    
#now see if improvement with JD.Second

var.seq<-c("no.users + Business", 
           "no.users")

conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)))
auc<- double(2)
colours <- c("red", "yellow")

#make fresh random sample of data
set.seed(200)
sample<- sample(1:nrow(all9c), 2/3*nrow(all9c), replace=F)
train<- all9c[sample,]
test<- all9c[-sample,]

for(i in 1:1){
        
        formula<- paste("b.rpdol~ Discipline + pc.pro + inv.mlsto.log + pc.majpos.log + majority.pos + b.timespan.cbrt + client.totinv.log + Billing.Type", var.seq[i], sep= " + ")
        
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 10000,
                 shrinkage=0.001,interaction.depth=4,
                 n.minobsinnode = 20)
        pred<- predict(fit, test, n.tree=10000, type="response")

        #pROC
        pred.p<- roc(test$b.rpdol, pred)
        
        if(i==1){
                plot.roc(pred.p, col= colours[i], print.thres = T)
                
        } else{
                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = T)
        }
        
        auc[i]<- auc(pred.p)
        
        #confusion matrix
        conf.mat<- coords(pred.p, "best")
        pred1<- ifelse(pred>= conf.mat[1], 1, 0)
        temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
auc

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=1:2, "auc" = auc[1:2])
ggplot(shrink, aes(x=shrinkage, y=auc)) + geom_line() + geom_point()+ geom_line()

# Business helps!!!!!!!!

```
    
Tune again with caret package, INCLUDING Business category. 
    
```{r, echo=FALSE}

# set seeds
set.seed(123)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
## For the last model:
seeds[[51]] <- sample.int(1000, 1)

#create test train
sample<- sample(1:nrow(all9c), 2/3*nrow(all9c), replace=F)
train<- all9c[sample,]
test<- all9c[-sample,]

# first tune shrinkage and number of trees
train2<- train
train2$b.rpdol<- as.factor(train2$b.rpdol)
levels(train2$b.rpdol)[levels(train2$b.rpdol)=="0"] <- "profit"
levels(train2$b.rpdol)[levels(train2$b.rpdol)=="1"] <- "loss"

formula<- "b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + inv.mlsto.log +
client.totinv.log + pc.majpos.log + Billing.Type + majority.pos + Business"

cv.Control <- trainControl(method = "cv",
                           number = 5,
                           seeds= seeds
                           , summaryFunction=twoClassSummary,
                           classProbs=TRUE
                           )
gbmGrid <- expand.grid(shrinkage = c(0.001),
                       n.trees = c(10000), interaction.depth = c(1,2,3,4,5),
                       n.minobsinnode = c(10,20,30,40))
set.seed(2)
gbmFit <- train(as.formula(formula), data= train2,
                 method = "gbm", trControl = cv.Control, verbose = FALSE,
                 bag.fraction = 0.5, tuneGrid = gbmGrid,
                metric = 'ROC',
                na.action = na.pass)

plot(gbmFit)
plot(gbmFit, plotType = 'level')

# therefore:
# shrinkage = 0.001
# no trees = 10000
# min terminal node size = 20
# max tree depth = 4

# AUC approx 0.770 in 5 fold CV 

```

Does the boosted tree perform better with a subset of the data based on later years?

```{r, echo=FALSE}



formula<- "b.rpdol ~ Discipline + pc.pro + b.timespan.cbrt + no.users + inv.mlsto.log +
client.totinv.log + pc.majpos.log + majority.pos + Business"

Year.lim <- c(2000, 2004, 2006, 2008, 2009, 2010)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)), '6' = data.frame(c(0,0), c(0,0)))
auc<- rep(list(rep(0,8)),6)
colours <- c("red", "yellow", "green", "blue", "purple", "pink")

        
for(i in 1:6){
        
        #set up train and test
        set.seed = 100
        subset = all9c %>% filter(Year> Year.lim[i])
        sample<- sample(1:nrow(subset), .75*nrow(subset), replace=F)
        train<- all9c[sample,]
        test<- all9c[-sample,]
        #set up 10 fold cv using caret function
        folds <- createFolds(train$b.rpdol, k=8, list=T, returnTrain=F)
        #now have folds[[1]] through folds[[5]] list of rows to exclude 

        #see if ROC improves
        
        for (j in 1:8){
                #make boosted tree
                fit<-gbm(as.formula(formula), 
                         data=train[-folds[[j]],], distribution="bernoulli", n.trees= 10000,
                         shrinkage=0.001,interaction.depth=4,
                         n.minobsinnode = 20)
                
                pred<- predict(fit, train[folds[[j]], ], n.tree=10000, type="response")
                
                        #pROC
                        pred.p<- roc(train[folds[[j]],'b.rpdol'], pred)
        
                        if(i==1 & j==1){
                                plot.roc(pred.p, col= colours[i], print.thres = F)
                
                        } else{
                                plot.roc(pred.p, col= colours[i], add=TRUE , print.thres = F)
                        }
        
                        auc[[i]][j]<- auc(pred.p)
        
                        #confusion matrix
#                         conf.mat<- coords(pred.p, "best")
#                         pred1<- ifelse(pred>= conf.mat[1], 1, 0)
#                         temp2<- data.frame('pred' = pred1, 'ans' = test$b.rpdol)
#                         conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
#                         cat(i," ")  
                        cat(j, " ")
        }
        
        
        print(i)        
}

conf.matrix
auc
saveRDS(auc, 'C:/Users/n9232371/Documents/github/consult/finalwriteup/images/boosted tree/Year_vary_cv.rds')

a<- ggplot(melt(auc), aes(x= factor(L1), y= value)) + geom_boxplot()

# All years are required! Gives the best result
```

    