---
title: "Boosted"
author: "Amy Cook"
date: "October 19, 2015"
output: word_document
---


```{r, load library and packages, include= FALSE}
library("plyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library("dplyr",lib = 'C:/Progra~1/R/R-3.2.1/library')
library('magrittr',lib='C:/Progra~1/R/R-3.2.1/library')
library('reshape2',lib='C:/Progra~1/R/R-3.2.1/library')
library('ggplot2',lib='C:/Progra~1/R/R-3.2.1/library')
library('gbm', lib = 'C:/Progra~1/R/R-3.2.2/library')
library('caret', lib = 'C:/Progra~1/R/R-3.2.2/library')

# setwd("C:/Users/n9232371/Documents/Consultbusiness/data")
```

Read in data. Change return.pdol to 0,1 variable

```{r, echo=FALSE}
all8a<- read.csv('C:/Users/n9232371/Documents/Consultbusiness/data/all8a.csv')[,-1]

all8a$b.rpdol<- all8a$return.pdol 
all8a$b.rpdol<- ifelse(all8a$return.pdol<=0, 0, 1)

summary(all8a$b.rpdol)

all9a<- all8a
```


Create test and train sample

```{r, echo=FALSE}
set.seed(300)
sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
train<- all9a[sample,]
test<- all9a[-sample,]

```

Try building a boosted tree wihtout any training

```{r, echo=FALSE}
boost.1 <- gbm(b.rpdol~.- mlsto - return.pdol-Year-code.contact - code.client, 
               distribution = "bernoulli", data=train,
          n.trees = 10000,
          shrinkage = 0.001,
          interaction.depth = 1,
          n.minobsinnode = 10,
          bag.fraction = 0.5)

print(boost.1)
summary(boost.1)

#look at first tree
pretty.gbm.tree(boost.1,9000)
boost.1$var.names

#view OOB error plot
gbm.perf(boost.1, method="OOB", plot.it=TRUE)
#error still rapidly reducing at 10000 trees

#partial dependence plots:
i=8
a<- plot.gbm(boost.1, i, type="response")
a
#Discipline, Business, inv.mlsto.log, client.totinv.log, Billing.Type dont matter?


```

First tune shrinkage and plot. Get rid of year because in future, won't have historical data on year..

```{r, echo=FALSE}
formula = "b.rpdol~.- mlsto - return.pdol-Year-code.client-code.contact"

errors<- double(4)
shrink.seq<-c(.001,.005,.01,.05)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
for(i in 1:4){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees=5000,
                 shrinkage=shrink.seq[i],interaction.depth=3)
        pred<- predict(fit, test, n.tree=5000, type="response")
        pred<- ifelse(pred>=0.5,1,0)
        temp<- ifelse(pred == test$b.rpdol, 0,1)
        errors[i]<- sum(temp)/nrow(test)
        
        #confusion matrix
        temp2<- data.frame('pred' = pred, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
errors

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=shrink.seq, "error" = errors[1:4])
ggplot(shrink, aes(x=shrinkage, y=error)) + geom_line() + geom_point()+ geom_line()

```

Try caret method of tuning

```{r, echo=FALSE, include=FALSE}
# must turn outcome into two level factor
# all9a$f.rpdol<- as.factor(all9a$b.rpdol)
# set.seed(100)
# sample<- sample(1:nrow(all9a), 2/3*nrow(all9a), replace=F)
# train<- all9a[sample,]
# test<- all9a[-sample,]
# 
# # set seeds
# set.seed(123)
# seeds <- vector(mode = "list", length = 51)
# for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
# ## For the last model:
# seeds[[51]] <- sample.int(1000, 1)
# 
# # first tune shrinkage and number of trees
# cv.Control <- trainControl(method = "cv",
#                            number = 10,
#                            seeds= seeds)
# gbmGrid <- expand.grid(shrinkage = c(.001,.005),
#                        n.trees = c(2000), interaction.depth = 2,
#                        n.minobsinnode = 10)
# set.seed(2)
# gbmFit <- train(f.rpdol~.- mlsto - return.pdol-Year, data= train,
#                  method = "gbm", trControl = cv.Control, verbose = FALSE,
#                  bag.fraction = 0.5, tuneGrid = gbmGrid,
#                 metric = 'kappa')
# 
# plot(gbmFit)
```

Caret package is not working, tune manually:

```{r, echo=FALSE}

#shrinkage should be 0.001, now find number of trees
errors<- double(5)
ntree.seq<-c(4000,8000,10000,12000,14000)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)))
for(i in 1:5){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= ntree.seq[i],
                 shrinkage=0.001,interaction.depth=3)
        pred<- predict(fit, test, n.tree=ntree.seq[i], type="response")
        pred<- ifelse(pred>=0.5,1,0)
        temp<- ifelse(pred == test$b.rpdol, 0,1)
        errors[i]<- sum(temp)/nrow(test)
        
        #confusion matrix
        temp2<- data.frame('pred' = pred, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
errors

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=ntree.seq, "error" = errors[1:5])
ggplot(shrink, aes(x=shrinkage, y=error)) + geom_line() + geom_point()+ geom_line()

#10000 trees seems best?

#----------------------------------------------------------------------

#shrinkage should be 0.001, 10000 trees, now find interaction.depth
errors<- double(5)
int.seq<-c(1,2,4,6,8)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)),
                   '5' = data.frame(c(0,0), c(0,0)))
for(i in 1:5){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 10000,
                 shrinkage=0.001,interaction.depth=int.seq[i])
        pred<- predict(fit, test, n.tree=10000, type="response")
        pred<- ifelse(pred>=0.5,1,0)
        temp<- ifelse(pred == test$b.rpdol, 0,1)
        errors[i]<- sum(temp)/nrow(test)
        
        #confusion matrix
        temp2<- data.frame('pred' = pred, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
errors

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=int.seq, "error" = errors[1:5])
ggplot(shrink, aes(x=shrinkage, y=error)) + geom_line() + geom_point()+ geom_line()

## use interaction.depth = 4?



#------------------------------------------------
# now tune min obs in node
errors<- double(4)
obs.seq<-c(10,20,40,50)
conf.matrix<- list('1' = data.frame(c(0,0), c(0,0)), '2' = data.frame(c(0,0), c(0,0)),
                   '3' = data.frame(c(0,0), c(0,0)), '4' = data.frame(c(0,0), c(0,0)))
for(i in 1:4){
        fit<-gbm(as.formula(formula), 
                 data=train, distribution="bernoulli", n.trees= 10000,
                 shrinkage=0.001,interaction.depth=4,
                 n.minobsinnode = obs.seq[i])
        pred<- predict(fit, test, n.tree=10000, type="response")
        pred<- ifelse(pred>=0.5,1,0)
        temp<- ifelse(pred == test$b.rpdol, 0,1)
        errors[i]<- sum(temp)/nrow(test)
        
        #confusion matrix
        temp2<- data.frame('pred' = pred, 'ans' = test$b.rpdol)
        conf.matrix[[i]] = as.data.frame.matrix(table(temp2$pred, temp2$ans))
        cat(i," ")
}

conf.matrix
errors

#create new data frame 'shrink' to place trees and errors vectors together, then plot:
shrink<- data.frame("shrinkage"=obs.seq, "error" = errors[1:4])
ggplot(shrink, aes(x=shrinkage, y=error)) + geom_line() + geom_point()+ geom_line()




